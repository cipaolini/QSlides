[
  {
    "objectID": "BKL_22/abstract.html",
    "href": "BKL_22/abstract.html",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "",
    "text": "[The waiter]subject [gave]verb [my cousin]recipient [some pizza]theme\n[The waiter]subject [gave]verb [some pizza]theme [to my cousin]recipient\n\nMany quantitative studies focused on the more traditional formal predictors, such as structural complexity of constituents, pronominality, constituent length (e.g. Szmrecsanyi et al 2017) to explain the choice of one variant over the other. In contrast, semantic properties have been largely neglected in variationist alternation research due to its perceived cost efficiency. Manually annotating for semantic predictors is labor-intensive, time-consuming, and challenging to perform systematically and objectively, while not promising much in terms of explanatory power. Röthlisberger et al. (2017) has tried to tackle this issue via inclusion of lexical random effects in regression analysis, but while this method works reasonably well to increase the goodness of fit of regression models, it does not contribute to explaining the phenomenon. In this presentation, we introduce a potential solution to this issue: automatically generated semantic predictors using distributional models of meaning (Lenci 2018). The objectives of this research are on the one hand to determine the importance of semantic properties of the lexical context for predicting variant choice, and on the other hand whether and what they add to the explanatory power of traditional formal predictors. To accomplish our aims, the heads of the noun phrases that take the role of theme and recipient (e.g. pizza and cousin, respectively, in (i)) are represented in terms of their association strength to other items in a corpus. Based on such a numerical profile, we can cluster both the theme heads and the recipient heads and use the resulting classes as categorical predictors in a regression model. The approach is applied on a dataset of 1200 observations of the alternation for give in Spoken American English (Bresnan et al. 2007); the heads are modelled with data from the Spoken COCA (Davies 2008 - , ~127 million words).\nThe preliminary findings suggest that semantic clusters have significant predictive power, but traditional predictors appear to be subtly more powerful than type-level semantic predictors. Nevertheless, lexical effects emerge as the most important features for both theme and recipients, opening the research to further applications and developments.\n\n\n\n\n\n\n\nReferences\nBresnan, J., Cueni, A., Nikitina, T., Baayen, H., 2007. Predicting the Dative Alternation, in: Bouma, G., Kraemer, I., Zwarts, J. (Eds.), Cognitive Foundations of Interpretation. Royal Netherlands Academy of Science, Amsterdam, pp. 69–94.\nDavies, M., 2008-. The Corpus of Contemporary American English (COCA). Available online at https://www.englishcorpora.org/coca/. Lenci, A., 2018. Distributional models of word meaning. Annual review of Linguistics, 4, 151-171.\nRöthlisberger, M., Grafmiller, J., Szmrecsanyi, B., 2017. Cognitive indigenization effects in the English dative alternation. Cogn. Linguist. 28, 673–710.\nSzmrecsanyi, B., Grafmiller, J., Bresnan, J., Rosenbach, A., Tagliamonte, S., Todd, S., 2017. Spoken syntax in a comparative perspective: The dative and genitive alternation in varieties of English. Glossa J. Gen. Linguist. 2, 86."
  },
  {
    "objectID": "BKL_22/index.html#outline",
    "href": "BKL_22/index.html#outline",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Outline",
    "text": "Outline\n\nIntroduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nInsights and further research\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "BKL_22/index.html#the-dative-alternation",
    "href": "BKL_22/index.html#the-dative-alternation",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "The dative alternation",
    "text": "The dative alternation\n\n\nDitransitive dative variant\n\n\n[The waiter]subject [gave]verb [my cousin]recipient [some pizza]theme\n\n\nPrepositional dative variant\n\n\n[The waiter]subject [gave]verb [some pizza]theme [to my cousin]recipient\n\nLet’s dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as “two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker”.\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order)."
  },
  {
    "objectID": "BKL_22/index.html#modelling-grammatical-alternations",
    "href": "BKL_22/index.html#modelling-grammatical-alternations",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Modelling grammatical alternations",
    "text": "Modelling grammatical alternations\nIn previous literature, focus on three kinds of predictors:\n\nFormal predictors: e.g., structural complexity of constituents (e.g., presence of heavy postmodification), pronominality, and constituent length (in words, syllables, or similar)\nInformation status‐related predictors: e.g., givenness\nSemantic, coarse-grained, higher-level predictors: e.g., animacy (i.e., annotate for a binary distinction between animate and inanimate recipients – see Bresnan et al. 2007)\n\n\n\n\n\nTo explore the correlation of choices between the two variants are both implemented language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy. This is because (next slide)"
  },
  {
    "objectID": "BKL_22/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "href": "BKL_22/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Role of semantic predictors in alternation predictions",
    "text": "Role of semantic predictors in alternation predictions\nAnnotating for semantics is labor‐intensive and challenging to perform objectively.\n\n\nWhat role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.\n\n\n\nWe assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.\n\n\nAnnotating for semantics is labor‐intensive and time-consuming, and it’s challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice."
  },
  {
    "objectID": "BKL_22/index.html#automatically-generated-semantic-predictors",
    "href": "BKL_22/index.html#automatically-generated-semantic-predictors",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Automatically-generated semantic predictors",
    "text": "Automatically-generated semantic predictors\n\nOur suggestion: automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).\n\n\n\nNew inputs from DMSs: more data, less annotation\nDistributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.\n\n\n\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items."
  },
  {
    "objectID": "BKL_22/index.html#examples-recipient-type-lemmas",
    "href": "BKL_22/index.html#examples-recipient-type-lemmas",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Examples: recipient type-lemmas",
    "text": "Examples: recipient type-lemmas\n\nDAT-4100\n\n\n\n[if I]subject [gave]verb [it]theme [to the government]recipient they would just waste it.\n\n\n\nDAT-4067\n\n\n\n[The judge]subject [will usually, uh, give]verb [custody]theme [to the mother]recipient ninety-seven percent of the time.\n\n\n\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (read the examples)."
  },
  {
    "objectID": "BKL_22/index.html#matrix-of-recipients",
    "href": "BKL_22/index.html#matrix-of-recipients",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Matrix of recipients",
    "text": "Matrix of recipients\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n-1.23\n3.23\n0.21\nNA\n2.59\n\n\n\nmother/nn\n\n4.36\nNA\n1.65\n2.89\nNA\n\n\n\nadvance/nn\n\n-2.32\n2.09\nNA\n-0.59\n3.67\n\n\n\n\n\nDative alternation dataset from Szmrecsanyi et al. (2017), which covers N = 4,136 dative observations in contemporary spoken English (Switchboard corpus)\nCorpus for building DMSs: Corpus of Contemporary American English (COCA), spoken register (ca. 127 million tokens)\n\n\n\nIn this first part of the study, we implemented what we call a type-level model.\nLet’s consider only the group of recipients, here exemplified by government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-words from the recipient slot: the aggregation of the frequencies between the TW and the CW constitutes a word-type vector. What you see here are raw frequencies transformed, or better, weighted using association strength measures, such as PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\n(read about the data set)"
  },
  {
    "objectID": "BKL_22/index.html#from-predictors-to-clouds-cloud-of-recipients",
    "href": "BKL_22/index.html#from-predictors-to-clouds-cloud-of-recipients",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "From predictors to clouds: cloud of recipients",
    "text": "From predictors to clouds: cloud of recipients\n\nFigure 1: Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.\nBuilding the semantic predictors using DS means identifying the central member of the cluster (called medoid) from the data and grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "BKL_22/index.html#from-predictor-to-clouds-cloud-of-themes",
    "href": "BKL_22/index.html#from-predictor-to-clouds-cloud-of-themes",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "From predictor to clouds: cloud of themes",
    "text": "From predictor to clouds: cloud of themes\n\nFigure 2: Scatterplot of themes lemmas. Colors are clusters, labelled by their central members."
  },
  {
    "objectID": "BKL_22/index.html#from-clouds-to-distributional-semantic-predictors",
    "href": "BKL_22/index.html#from-clouds-to-distributional-semantic-predictors",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "From clouds to distributional semantic predictors",
    "text": "From clouds to distributional semantic predictors\n\nEach grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor.\n\nThe prediction of the dative variants is based on the membership of the recipient/theme type-lemma in a particular semantic cluster.\n\nTo summarize, each grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor. In what follows, we predict dative choices based on the membership of the recipients/themes in a particular semantic cluster by using two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis."
  },
  {
    "objectID": "BKL_22/index.html#random-forest-of-traditional-and-distributional-predictors",
    "href": "BKL_22/index.html#random-forest-of-traditional-and-distributional-predictors",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Random forest of traditional and distributional predictors",
    "text": "Random forest of traditional and distributional predictors\n\n\nCRF ia a recursive partitioning method based on conditional inference trees: individual trees are ‘grown’, and their predictions are averaged. This statistical method can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "BKL_22/index.html#inside-the-most-important-clouds",
    "href": "BKL_22/index.html#inside-the-most-important-clouds",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Inside the most important clouds",
    "text": "Inside the most important clouds\n\nTheme.one:\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\nit/pp, that/dd, theory/nn, thing/nn"
  },
  {
    "objectID": "BKL_22/index.html#another-look-at-the-clouds-regression-modeling",
    "href": "BKL_22/index.html#another-look-at-the-clouds-regression-modeling",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Another look at the clouds: regression modeling",
    "text": "Another look at the clouds: regression modeling\nDifference of fixed/random effects between models: in what extent the contribution of the predictors change in the models?\n\n\n\nDelta c-values\ndiffprop\noddsratio\ndifflogits\n\n\n\n\nRM only trad predictors\n0.007643931\n2.127902\n0.7551367\n\n\nRM trad+sem predictors\n-0.009705108\n0.6100782\n-0.4941681\n\n\n\n\nComputating the delta c-value, we have been able to check the Difference of fixed/random effects between models: in what extent the contribution of the predictors change in the models? (see with mariana)"
  },
  {
    "objectID": "BKL_22/index.html#another-look-at-the-clouds-regression-modeling-1",
    "href": "BKL_22/index.html#another-look-at-the-clouds-regression-modeling-1",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Another look at the clouds: regression modeling",
    "text": "Another look at the clouds: regression modeling\nDifference of fixed/random effects between models: in what extent the contribution of the predictors change in the models?\n\n\n\nDelta c-values\ndiffprop\noddsratio\ndifflogits\n\n\n\n\nRM only trad predictors\n0.007643931\n2.127902\n0.7551367\n\n\nRM trad+sem predictors\n-0.009705108\n0.6100782\n-0.4941681\n\n\n\n\nfrom Speelman"
  },
  {
    "objectID": "BKL_22/index.html#take-home-messages",
    "href": "BKL_22/index.html#take-home-messages",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Take-home messages",
    "text": "Take-home messages\n\n“You shall choose a variant by the company it keeps!”\n(semi-cit. Firth 1955)\n\n\nOverall, there are some distributional semantics clusters that are very predictive of the alternation\nStatistical analyses show how traditional predictors outperform distributional semantics predictors in terms of model performances\n\n\n\nbased on the c-value"
  },
  {
    "objectID": "BKL_22/index.html#directions-for-future-research",
    "href": "BKL_22/index.html#directions-for-future-research",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Directions for future research",
    "text": "Directions for future research\n\nOther alternations:\n\nClausal complementation alternation in the history of English\nProgressive alternation in Italian\n\nExperiment with token-level modeling\n\nsee Montes (2021)\n\n\n\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context\n\n\n\n\nBKL - Liège, 2022-10-21"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Slides by Chiara Paolini",
    "section": "",
    "text": "1 Linguists’ Day 2022 (CBL-BKL)\nThese are the slides presented at the Belgian Society of Linguistics on 21-10-2022 with Benedikt Szmrecsanyi and Mariana Montes. Read the abstract\n\n\n\n\n\n2 QLVL Colloquium\nThese are the slides presented at the QLVL Colloquium on 4-11-2022. Read the abstract\n\n\n\n\n\n3 Taal en Tongval\nThese are the slides presented at the Taal en Tongval Colloquium on 18-11-2022 with Benedikt Szmrecsanyi and Mariana Montes. Read the abstract\n\n\n\n\n\n4 Methods in corpus linguistics: a case study with logistic regression\nThese are the slides of the lecture held on 18-11-2022 for the Methods in Corpus Linguistics master course. Here the step-by-step case study"
  },
  {
    "objectID": "BKL_22/index.html#another-look-at-the-clouds-regression-modelling",
    "href": "BKL_22/index.html#another-look-at-the-clouds-regression-modelling",
    "title": "Using distributional semantics to annotate for semantic predictors",
    "section": "Another look at the clouds: regression modelling",
    "text": "Another look at the clouds: regression modelling\nc-values of regression model with only traditional and traditional+semantic predictors\n\n\n\n\n\n\n\n\n\nRM only trad predictors\nRM trad+sem predictors\n\n\n\n\nC-value for fixed effects\n0.9844213\n0.9747162\n\n\nC-value for fixed and random effects\n0.9854791\n0.993123\n\n\n\n\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\nWe computed the c-value (Concordance index C: goodness of fit) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the Tmodel suggests how the traditional p performs better alone, than in combination with the semantic ones."
  },
  {
    "objectID": "QLVL_colloquium/abstract.html",
    "href": "QLVL_colloquium/abstract.html",
    "title": "Annotation goes distributional:",
    "section": "",
    "text": "References\nBresnan, J., Cueni, A., Nikitina, T., Baayen, H., 2007. Predicting the Dative Alternation, in: Bouma, G., Kraemer, I., Zwarts, J. (Eds.), Cognitive Foundations of Interpretation. Royal Netherlands Academy of Science, Amsterdam, pp. 69–94.\nDavies, M., 2008-. The Corpus of Contemporary American English (COCA). Available online at https://www.englishcorpora.org/coca/. Lenci, A., 2018. Distributional models of word meaning. Annual review of Linguistics, 4, 151-171.\nLenci, A., 2018. Distributional Models of Word Meaning. Annual Review of Linguistics 4 (1): 151–71.\nRöthlisberger, M., Grafmiller, J., Szmrecsanyi, B., 2017. Cognitive indigenization effects in the English dative alternation. Cogn. Linguist. 28, 673–710.\nSzmrecsanyi, B., Grafmiller, J., Bresnan, J., Rosenbach, A., Tagliamonte, S., Todd, S., 2017. Spoken syntax in a comparative perspective: The dative and genitive alternation in varieties of English. Glossa J. Gen. Linguist. 2, 86."
  },
  {
    "objectID": "QLVL_colloquium/index.html#outline",
    "href": "QLVL_colloquium/index.html#outline",
    "title": "Annotation goes distributional:",
    "section": "Outline",
    "text": "Outline\n\nIntroduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nConclusions\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "QLVL_colloquium/index.html#the-dative-alternation",
    "href": "QLVL_colloquium/index.html#the-dative-alternation",
    "title": "Annotation goes distributional:",
    "section": "The dative alternation",
    "text": "The dative alternation\n\n1) a. Ditransitive dative variant\n\n[The waiter]subject [gave]verb [my cousin]recipient [some pizza]theme\n\n  b. Prepositional dative variant\n\n[The waiter]subject [gave]verb [some pizza]theme [to my cousin]recipient\n\nLet’s dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as “two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker”.\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order)."
  },
  {
    "objectID": "QLVL_colloquium/index.html#modelling-grammatical-alternations",
    "href": "QLVL_colloquium/index.html#modelling-grammatical-alternations",
    "title": "Annotation goes distributional:",
    "section": "Modelling grammatical alternations",
    "text": "Modelling grammatical alternations\nIn previous literature, focus on three kinds of predictors:\n\nFormal predictors: e.g., structural complexity of constituents (e.g., presence of heavy postmodification), pronominality, and constituent length (in words, syllables, or similar)\nInformation status‐related predictors: e.g., givenness\nSemantic, coarse-grained, higher-level predictors: e.g., animacy (i.e., annotate for a binary distinction between animate and inanimate recipients – see Bresnan et al. 2007)\n\n\n\n\n\nTo explore the correlation of choices between the two variants are both implemented language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal, top-down predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy, which can be defined as a top-down notion. This is because (next slide)"
  },
  {
    "objectID": "QLVL_colloquium/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "href": "QLVL_colloquium/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "title": "Annotation goes distributional:",
    "section": "Role of semantic predictors in alternation predictions",
    "text": "Role of semantic predictors in alternation predictions\nAnnotating for semantics is labor‐intensive and challenging to perform objectively.\n\n\nWhat role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.\n\n\n\nWe assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.\n\n\nAnnotating for semantics is labor‐intensive and time-consuming, and it’s challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice."
  },
  {
    "objectID": "QLVL_colloquium/index.html#automatically-generated-semantic-predictors",
    "href": "QLVL_colloquium/index.html#automatically-generated-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "Automatically-generated semantic predictors",
    "text": "Automatically-generated semantic predictors\n\nWe use automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).\n\n\n\nNew inputs from DMSs: more data, less annotation\nDistributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.(Lenci 2018)\n\n\n\nA completely bottom-up approach to annotation.\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items."
  },
  {
    "objectID": "QLVL_colloquium/index.html#examples-recipient-type-lemmas",
    "href": "QLVL_colloquium/index.html#examples-recipient-type-lemmas",
    "title": "Annotation goes distributional:",
    "section": "Examples: recipient type-lemmas",
    "text": "Examples: recipient type-lemmas\n\n2) a. DAT-4100\n\n\n\n[if I]subject [gave]verb [it]theme [to the government]recipient they would just waste it.\n\n\n\n  b. DAT-4067\n\n\n\n[The judge]subject [will usually, uh, give]verb [custody]theme [to the mother]recipient ninety-seven percent of the time.\n\n\n\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (read the examples)."
  },
  {
    "objectID": "QLVL_colloquium/index.html#matrix-of-recipients",
    "href": "QLVL_colloquium/index.html#matrix-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Matrix of recipients",
    "text": "Matrix of recipients\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n-1.23\n3.23\n0.21\n0.0\n2.59\n\n\n\nmother/nn\n\n4.36\n0.0\n1.65\n2.89\n0.0\n\n\n\nadvance/nn\n\n-2.32\n2.09\n0.0\n-0.59\n3.67\n\n\n\n\n\nDative alternation dataset from Szmrecsanyi et al. (2017), which covers N = 1,190 dative observations in contemporary spoken American English (Switchboard corpus)\nCorpus for building DMSs: Corpus of Contemporary American English (COCA), spoken register (ca. 127 million tokens)\n\n\n\nIn this first part of the study, we implemented what we call a type-level model.\nLet’s consider only the group of recipients, here exemplified by the types government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-words from the recipient slot: the aggregation of the frequencies between the TW and the CW constitutes a word-type vector. What you see here are raw frequencies transformed, or better, weighted using association strength measures, such as PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\nThe values here represent the the attraction/the association strenght between the target word and a context word.\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\n(read about the data set)"
  },
  {
    "objectID": "QLVL_colloquium/index.html#from-predictors-to-clouds-cloud-of-recipients",
    "href": "QLVL_colloquium/index.html#from-predictors-to-clouds-cloud-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "From predictors to clouds: cloud of recipients",
    "text": "From predictors to clouds: cloud of recipients\n\nFigure 1: Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.\nBuilding the semantic predictors using DS means identifying the central member of the cluster (called medoid) from the data and grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "QLVL_colloquium/index.html#from-predictor-to-clouds-cloud-of-themes",
    "href": "QLVL_colloquium/index.html#from-predictor-to-clouds-cloud-of-themes",
    "title": "Annotation goes distributional:",
    "section": "From predictor to clouds: cloud of themes",
    "text": "From predictor to clouds: cloud of themes\n\nFigure 2: Scatterplot of themes lemmas. Colors are clusters, labelled by their central members."
  },
  {
    "objectID": "QLVL_colloquium/index.html#from-clouds-to-distributional-semantic-predictors",
    "href": "QLVL_colloquium/index.html#from-clouds-to-distributional-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "From clouds to distributional semantic predictors",
    "text": "From clouds to distributional semantic predictors\n\nEach grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor.\n\n\nThe prediction of the dative variants is based on the membership of the recipient/theme type-lemma in a particular semantic cluster.\n\n\nTo summarize, each grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor. In what follows, we predict dative choices based on the membership of the recipients/themes in a particular semantic cluster by using two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis."
  },
  {
    "objectID": "QLVL_colloquium/index.html#random-forest-of-traditional-and-distributional-predictors",
    "href": "QLVL_colloquium/index.html#random-forest-of-traditional-and-distributional-predictors",
    "title": "Annotation goes distributional:",
    "section": "Random forest of traditional and distributional predictors",
    "text": "Random forest of traditional and distributional predictors\n\n\nCRF ia a recursive partitioning method based on conditional inference trees: individual trees are ‘grown’, and their predictions are averaged. This statistical method can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-most-important-clouds",
    "href": "QLVL_colloquium/index.html#inside-the-most-important-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the most important clouds",
    "text": "Inside the most important clouds\n\nTheme.one:\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\nit/pp, that/dd, theory/nn, thing/nn"
  },
  {
    "objectID": "QLVL_colloquium/index.html#another-look-at-the-clouds-regression-modelling",
    "href": "QLVL_colloquium/index.html#another-look-at-the-clouds-regression-modelling",
    "title": "Annotation goes distributional:",
    "section": "Another look at the clouds: regression modelling",
    "text": "Another look at the clouds: regression modelling\n\nc-values of regression model with only traditional and traditional+semantic predictors\n\nIndex of concordance (c-value) = non-parametric measure of how well a statistical model fits a set of observations.\n\n\n\n\n\n\n\n\n\n\n\nRM only trad predictors\nRM trad+sem predictors\n\n\n\n\nC-value for fixed effects\n0.984\n0.975\n\n\nC-value for fixed and random effects\n0.985\n0.993\n\n\n\n\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\nWe computed the Concordance index C: goodness of fit (read the slide) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the T-model suggests how the traditional p performs better alone, than in combination with the semantic ones."
  },
  {
    "objectID": "QLVL_colloquium/index.html#take-home-messages",
    "href": "QLVL_colloquium/index.html#take-home-messages",
    "title": "Annotation goes distributional:",
    "section": "Take-home messages",
    "text": "Take-home messages\n\n“You shall choose a variant by the company it keeps!”\n(semi-cit. Firth 1955)\n\n\nOverall, there are some distributional semantics clusters that are very predictive of the alternation\nStatistical analyses show how traditional predictors outperform distributional semantics predictors in terms of model performances\n\n\n\nbased on the c-value"
  },
  {
    "objectID": "QLVL_colloquium/index.html#directions-for-future-research",
    "href": "QLVL_colloquium/index.html#directions-for-future-research",
    "title": "Annotation goes distributional:",
    "section": "Directions for future research",
    "text": "Directions for future research\n\nOther alternations:\n\nClausal complementation alternation in the history of English\nProgressive alternation in Italian\n\nExperiment with token-level modeling\n\nsee Montes (2021)\n\n\n\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context"
  },
  {
    "objectID": "QLVL_colloquium/index.html#clouds-of-recipients",
    "href": "QLVL_colloquium/index.html#clouds-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Clouds of recipients",
    "text": "Clouds of recipients\n\nFigure 1: Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.\nBuilding the semantic predictors using DS means identifying the central member of the cluster (called medoid) from the data and grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "QLVL_colloquium/index.html#clouds-of-themes",
    "href": "QLVL_colloquium/index.html#clouds-of-themes",
    "title": "Annotation goes distributional:",
    "section": "Clouds of themes",
    "text": "Clouds of themes\n\nFigure 2: Scatterplot of themes lemmas. Colors are clusters, labelled by their central members."
  },
  {
    "objectID": "QLVL_colloquium/index.html#random-forest",
    "href": "QLVL_colloquium/index.html#random-forest",
    "title": "Annotation goes distributional:",
    "section": "Random forest",
    "text": "Random forest\n\nRandom forest of traditional and distributional predictors\n\n\n\nCRF is a statistical method based CIF: trees are obtained though a finite series of recursive splits of the data in several dataset, based on predictors which better distinguish between the different values of the response variable. In CRF, the individual trees are ‘grown’ together, and their predictions are averaged. This statistical method can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "QLVL_colloquium/index.html#thank-you",
    "href": "QLVL_colloquium/index.html#thank-you",
    "title": "Annotation goes distributional:",
    "section": "Thank you!",
    "text": "Thank you!\n\n\n\nchiara.paolini@kuleuven.be\n\n\n\n\n\nQLVL colloquium - Leuven, 2022-11-4"
  },
  {
    "objectID": "QLVL_colloquium/index.html#thoughts-comments-suggestions",
    "href": "QLVL_colloquium/index.html#thoughts-comments-suggestions",
    "title": "Annotation goes distributional:",
    "section": "Thoughts? Comments? Suggestions?",
    "text": "Thoughts? Comments? Suggestions?\n\n\nchiara.paolini@kuleuven.be"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html",
    "href": "QLVL_colloquium/index_pdf.html",
    "title": "Annotation goes distributional:",
    "section": "",
    "text": "Introduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nInsights and further research\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#the-dative-alternation",
    "href": "QLVL_colloquium/index_pdf.html#the-dative-alternation",
    "title": "Annotation goes distributional:",
    "section": "The dative alternation",
    "text": "The dative alternation\n\n\n\na) Ditransitive dative variant\n\n\n[The waiter]subject [gave]verb [my cousin]recipient [some pizza]theme\n\n\n\nb) Prepositional dative variant\n\n\n[The waiter]subject [gave]verb [some pizza]theme [to my cousin]recipient\n\nLet’s dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as “two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker”.\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order)."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#modelling-grammatical-alternations",
    "href": "QLVL_colloquium/index_pdf.html#modelling-grammatical-alternations",
    "title": "Annotation goes distributional:",
    "section": "Modelling grammatical alternations",
    "text": "Modelling grammatical alternations\nIn previous literature, focus on three kinds of predictors:\n\nFormal predictors: e.g., structural complexity of constituents (e.g., presence of heavy postmodification), pronominality, and constituent length (in words, syllables, or similar)\nInformation status‐related predictors: e.g., givenness\nSemantic, coarse-grained, higher-level predictors: e.g., animacy (i.e., annotate for a binary distinction between animate and inanimate recipients – see Bresnan et al. 2007)\n\n\n\n\n\nTo explore the correlation of choices between the two variants are both implemented language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy. This is because (next slide)"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#role-of-semantic-predictors-in-alternation-predictions",
    "href": "QLVL_colloquium/index_pdf.html#role-of-semantic-predictors-in-alternation-predictions",
    "title": "Annotation goes distributional:",
    "section": "Role of semantic predictors in alternation predictions",
    "text": "Role of semantic predictors in alternation predictions\nAnnotating for semantics is labor‐intensive and challenging to perform objectively.\n\n\nWhat role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.\n\n\n\nWe assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.\n\n\nAnnotating for semantics is labor‐intensive and time-consuming, and it’s challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#automatically-generated-semantic-predictors",
    "href": "QLVL_colloquium/index_pdf.html#automatically-generated-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "Automatically-generated semantic predictors",
    "text": "Automatically-generated semantic predictors\n\nOur suggestion: automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).\n\n\n\nNew inputs from DMSs: more data, less annotation\nDistributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.\n\n\n\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#examples-recipient-type-lemmas",
    "href": "QLVL_colloquium/index_pdf.html#examples-recipient-type-lemmas",
    "title": "Annotation goes distributional:",
    "section": "Examples: recipient type-lemmas",
    "text": "Examples: recipient type-lemmas\n\n\nDAT-4100\n\n\n\n\n[if I]subject [gave]verb [it]theme [to the government]recipient they would just waste it.\n\n\n\n\nDAT-4067\n\n\n\n\n[The judge]subject [will usually, uh, give]verb [custody]theme [to the mother]recipient ninety-seven percent of the time.\n\n\n\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (read the examples)."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#matrix-of-recipients",
    "href": "QLVL_colloquium/index_pdf.html#matrix-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Matrix of recipients",
    "text": "Matrix of recipients\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\ngovernment/nn\n\n-1.23\n3.23\n0.21\n0.0\n2.59\n\n\n\nmother/nn\n\n4.36\n0.0\n1.65\n2.89\n0.0\n\n\n\nadvance/nn\n\n-2.32\n2.09\n0.0\n-0.59\n3.67\n\n\n\n\n\n\nDative alternation dataset from Szmrecsanyi et al. (2017), which covers N = 1,190 dative observations in contemporary spoken American English (Switchboard corpus)\nCorpus for building DMSs: Corpus of Contemporary American English (COCA), spoken register (ca. 127 million tokens)\n\n\n\nIn this first part of the study, we implemented what we call a type-level model.\nLet’s consider only the group of recipients, here exemplified by government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-words from the recipient slot: the aggregation of the frequencies between the TW and the CW constitutes a word-type vector. What you see here are raw frequencies transformed, or better, weighted using association strength measures, such as PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\n(read about the data set)"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#clouds-of-recipients",
    "href": "QLVL_colloquium/index_pdf.html#clouds-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Clouds of recipients",
    "text": "Clouds of recipients\n\n\n\n\nFigure 1: Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.\n\n\n\n\n\nBuilding the semantic predictors using DS means identifying the central member of the cluster (called medoid) from the data and grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#clouds-of-themes",
    "href": "QLVL_colloquium/index_pdf.html#clouds-of-themes",
    "title": "Annotation goes distributional:",
    "section": "Clouds of themes",
    "text": "Clouds of themes\n\n\n\n\nFigure 2: Scatterplot of themes lemmas. Colors are clusters, labelled by their central members."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#from-clouds-to-distributional-semantic-predictors",
    "href": "QLVL_colloquium/index_pdf.html#from-clouds-to-distributional-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "From clouds to distributional semantic predictors",
    "text": "From clouds to distributional semantic predictors\n\nEach grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor.\n\n\nThe prediction of the dative variants is based on the membership of the recipient/theme type-lemma in a particular semantic cluster.\n\n\nTo summarize, each grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor. In what follows, we predict dative choices based on the membership of the recipients/themes in a particular semantic cluster by using two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#random-forest",
    "href": "QLVL_colloquium/index_pdf.html#random-forest",
    "title": "Annotation goes distributional:",
    "section": "Random forest",
    "text": "Random forest\n\nRandom forest of traditional and distributional predictors\n\n\n\n\n\n\n\nCRF ia a recursive partitioning method based on conditional inference trees: individual trees are ‘grown’, and their predictions are averaged. This statistical method can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#inside-the-most-important-clouds",
    "href": "QLVL_colloquium/index_pdf.html#inside-the-most-important-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the most important clouds",
    "text": "Inside the most important clouds\n\nTheme.one:\n\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\n\nit/pp, that/dd, theory/nn, thing/nn"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#another-look-at-the-clouds-regression-modelling",
    "href": "QLVL_colloquium/index_pdf.html#another-look-at-the-clouds-regression-modelling",
    "title": "Annotation goes distributional:",
    "section": "Another look at the clouds: regression modelling",
    "text": "Another look at the clouds: regression modelling\n\nc-values of regression model with only traditional and traditional+semantic predictors\n\nChi-square (Χ2) goodness of fit (c-value) = non-parametric measure of how well a statistical model fits a set of observations.\n\n\n\n\n\n\n\n\n\n\nRM only trad predictors\nRM trad+sem predictors\n\n\n\nC-value for fixed effects\n0.9844213\n0.9747162\n\n\nC-value for fixed and random effects\n0.9854791\n0.993123\n\n\n\n\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\nWe computed the c-value (Concordance index C: goodness of fit) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the Tmodel suggests how the traditional p performs better alone, than in combination with the semantic ones."
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#take-home-messages",
    "href": "QLVL_colloquium/index_pdf.html#take-home-messages",
    "title": "Annotation goes distributional:",
    "section": "Take-home messages",
    "text": "Take-home messages\n\n“You shall choose a variant by the company it keeps!”\n(semi-cit. Firth 1955)\n\n\nOverall, there are some distributional semantics clusters that are very predictive of the alternation\nStatistical analyses show how traditional predictors outperform distributional semantics predictors in terms of model performances\n\n\n\nbased on the c-value"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#directions-for-future-research",
    "href": "QLVL_colloquium/index_pdf.html#directions-for-future-research",
    "title": "Annotation goes distributional:",
    "section": "Directions for future research",
    "text": "Directions for future research\n\n\nOther alternations:\n\nClausal complementation alternation in the history of English\nProgressive alternation in Italian\n\n\n\nExperiment with token-level modeling\n\nsee Montes (2021)\n\n\n\n\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context"
  },
  {
    "objectID": "QLVL_colloquium/index_pdf.html#thoughts-comments-suggestions",
    "href": "QLVL_colloquium/index_pdf.html#thoughts-comments-suggestions",
    "title": "Annotation goes distributional:",
    "section": "Thoughts? Comments? Suggestions?",
    "text": "Thoughts? Comments? Suggestions?\n\n\n\n\n\nchiara.paolini@kuleuven.be"
  },
  {
    "objectID": "QLVL_colloquium/index.html",
    "href": "QLVL_colloquium/index.html",
    "title": "Annotation goes distributional:",
    "section": "",
    "text": "Introduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nInsights and further research\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-clouds",
    "href": "QLVL_colloquium/index.html#inside-the-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the clouds",
    "text": "Inside the clouds\n\nTheme.one:\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\nit/pp, that/dd, theory/nn, thing/nn\n\n\n\n\n\nQLVL colloquium - Leuven, 2022-11-4"
  },
  {
    "objectID": "QLVL_colloquium/index.html#traditional-predictors",
    "href": "QLVL_colloquium/index.html#traditional-predictors",
    "title": "Annotation goes distributional:",
    "section": "Traditional predictors",
    "text": "Traditional predictors\n\n\nRecipient/Theme.type: The annotation distinguishes between the following categories: (1) noun phrase; (2) personal pronoun; (3) demonstrative pronoun; (4) impersonal pronoun.\nRecipient/Theme.definiteness: The annotation distinguishes between the following categories: (1) definite; (2) indefinite (3) definite proper noun.\nRecipient/Theme.animacy: The annotation distinguishes between the following categories: (1) human and animal; (2) collective; (3) temporal; (4) locative; (5) inanimate.\nLength.difference: The log difference between recipient and theme lengths (see Bresnan & Ford 2010).\nSemantics (of dative verb): (1) transfer; (2) communication; (3) abstract.\nRecipient/Theme.head: Head lexeme of both the theme and the recipient."
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-recipients-clouds",
    "href": "QLVL_colloquium/index.html#inside-the-recipients-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the recipients clouds",
    "text": "Inside the recipients clouds\n\n\nRecipient.advance: advance/vv, america/np, arm/nn, country/nn, europe/np, government/nn, iranian/nn, nation/nn, nicaragua/np, russia/np, schwartzkopf/np, troop/nn, vietnamese/np\nRecipient.wife: actress/nn, wife/nn, artist/nn, boss/nn, brother/nn, daughters/nn, friend/nn, husband/nn, kitty/np, ryan/np, sister/nn, thomases/np, trek/np\nRecipient.kids: agencies/nn\n2\n0.3443036000\nkids/nn\n16\nboys/nn\n2\n0.5979984000\nkids/nn\n20\nbusinesses/nn\n2\n0.4080819000\nkids/nn\n29\ncompanies/nn\n2\n0.5218712000\nkids/nn\n33\ncountries/nn\n2\n0.5652321000\nkids/nn\n37\ncriminals/nn\n2\n0.3794335700\nkids/nn\n42\ndogs/nn\n2\n0.5007423000\nkids/nn\n43\nemployees/nn\n2\n0.4876230700\nkids/nn\n56\nfriends/nn\n2\n0.4518532500\nkids/nn\n60\ngrandparents/nn\n2\n0.3280267400\nkids/nn\n66\nguys/nn\n2\n0.3701141800\nkids/nn\n70\nhorses/nn\n2\n0.3475765000\nkids/nn\n72\nhouses/nn\n2\n0.5975894300\nkids/nn\n82\nkids/nn\n2\n0.6238054000\nkids/nn\n95\nmembers/nn\n2\n0.4905860400\nkids/nn\n104\nparents/nn\n2\n0.5942958000\nkids/nn\n113\nrashad/np\n2\n0.3486836900\nkids/nn\n124\nsons/nn\n2\n0.5616968000\nkids/nn\n127\nstudents/nn\n2\n0.4786918000\nkids/nn\n135\nthings/nn\n2\n0.4555949600\nkids/nn\n141\ntwins/nn\n2\n0.5751200300\nkids/nn\n152\nwomen/nn\n\n\n\n\n\nQLVL colloquium - Leuven, 2022-11-4"
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-recipient-clouds",
    "href": "QLVL_colloquium/index.html#inside-the-recipient-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the recipient clouds",
    "text": "Inside the recipient clouds\n\n\nRecipient.advance: advance/vv, america/np, arm/nn, country/nn, europe/np, government/nn, iranian/nn, nation/nn, nicaragua/np, russia/np, schwartzkopf/np, troop/nn, vietnamese/np\nRecipient.wife: actress/nn, wife/nn, artist/nn, boss/nn, brother/nn, daughters/nn, friend/nn, husband/nn, kitty/np, ryan/np, sister/nn, thomases/np, trek/np\nRecipient.kids: agencies/nn, boys/nn, businesses/nn, companies/nn, countries/nn, criminals/nn, dogs/nn, employees/nn, friends/nn, grandparents/nn, guys/nn, horses/nn, houses/nn, kids/nn, members/nn, parents/nn, rashad/np, sons/nn, students/nn, things/nn, twins/nn, women/nn\nRecipient.people: people/nn, citizen/nn, folk/nn, individual/nn, other/nn, peasant/nn, public/nn, reader/nn, white/nn\nRecipient.person: person/nn, convict/nn. criminal/nn, guy/nn, man/nn, someone/pn, stranger/nn\nRecipient.court: court/nn, judge/nn, jury/nn, matter/nn, plaintiff/nn, topic/nn\nRecipient.business: business/nn, car/nn, charity/nn, company/nn, competitor/nn, employee/nn, enterprise/nn, environment/nn, food/nn, grower/nn, household/nn, management/nn, taxpayer/nn"
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-theme-clouds",
    "href": "QLVL_colloquium/index.html#inside-the-theme-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the theme clouds",
    "text": "Inside the theme clouds\n\n\nTheme.discount:\nTheme.book:\nTheme.bag:\nTheme.appeal:\nTheme.power:\nTheme.drug:\nTheme.doubt:\nTheme.half:\n\n\n\n\n\nQLVL colloquium - Leuven, 2022-11-4"
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-theme-clouds-1",
    "href": "QLVL_colloquium/index.html#inside-the-theme-clouds-1",
    "title": "Annotation goes distributional:",
    "section": "Inside the theme clouds #1",
    "text": "Inside the theme clouds #1\n\n\nTheme.discount: discount/nn, allowance/nn, amount/nn, bargain/nn, benefit/nn, bill/nn, billion/nn, bond/nn, bonus/nn, buck/nn, budget/nn, card/nn, cash/nn, cent/nn, checkbook/nn, contract/nn, credit-card/nn, credit/nn, cut/nn, deal/nn, degree/nn, discount/nn, dollar/nn, estimate/nn, extra/nn, incentive/nn, income/nn, insurance/nn, less/da, limit/nn, loan/nn, mail/nn, million/nn, minimum/nn, money/nn, more/da, mortgage/nn, much/da, nickel/nn, number/nn, offer/nn, outlook/nn, pay/nn, paycheck/nn, payment/nn, penny/nn, percent/nn, physical/nn, point/nn, price/nn, profit-sharing/nn, raise/nn, rate/nn, rating/nn, rebate/nn, refund/nn, salary/nn, scale/nn, stipend/nn, stock/nn, tariff/nn, tax-credit/nn, tax/nn, thumbs-down/nn, ticket/nn, tune-up/nn, veto/nn, warranty/nn, worth/nn\nTheme.book: book/nn, address/nn, appearance/nn, assistant/nn, band/nn, booklet/nn, book/nn, break/nn, broadcast/nn, cable/nn, call/nn, comedy/nn, demo/nn, description/nn, entertainment/nn, f/np, figure/nn, game/nn, glimpse/nn, graph/nn, headline/nn, history/nn, husband/nn, list/nn, literature/nn, look/nn, magazine/nn, meeting/nn, message/nn, music/nn, name/nn, new/jj, news/nn, newspaper/nn, novel/nn, paper/nn, picture/nn, present/nn, rap/nn, report/nn, run/nn, rush/nn, screen/nn, sermon/nn, song/nn, speech/nn, story/nn, subscription/nn, subtitle/nn, synopsis/nn, talk/nn, tape/nn, television/nn, this/dd, tile/nn, topic/nn, trip/nn, tv/nn, update/nn, warning/nn, world/nn\nTheme.bag: bag/nn, amp/nn, arm/nn, azalea/nn, backpack/nn, ball/nn, bath/nn, bedroom/nn, bike/nn, bin/nn, body/nn, bulb/nn, can/nn, car/nn, coat/nn, deer/nn, dog/nn, glass/nn, gun/nn, inn/nn, junkyard/nn, knife/nn, light/nn, paint/nn, pair/nn, parking/nn, pt/nn, pipe/nn, plate/nn, pony/nn, puppy/nn, purse/nn, rabbit/nn, ride/nn, ring/nn, room/nn, seat/nn, shirts/nn, shirt/nn, shower/nn, spec/nn, spot/nn, suit/nn, sweatshirt/nn, tub/nn, uniform/nn, walking/nn"
  },
  {
    "objectID": "QLVL_colloquium/index.html#inside-the-theme-clouds-2",
    "href": "QLVL_colloquium/index.html#inside-the-theme-clouds-2",
    "title": "Annotation goes distributional:",
    "section": "Inside the theme clouds #2",
    "text": "Inside the theme clouds #2\n\n\nTheme.appeal: appeal/nn, appointment/nn, choice/nn, custody/nn, decision/nn, demonstration/nn, demonstrator/nn, detention/nn, election/nn, felony/nn, jurisdiction/nn, law/nn, nomination/nn, order/nn, parole/nn, penalty/nn, punishment/nn, release/nn, right/nn, rule/nn, sentence/nn, trial/nn, verdict/nn\nTheme.power: power/nn, ability/nn, activity/nn, advantage/nn, ambition/nn, attention/nn, balance/nn, chance/nn, command/nn, concession/nn, consistency/nn, control/nn, credibility/nn, dignity/nn, direction/nn, flexibility/nn, focus/nn, foothold/nn, freedom/nn, goal/nn, importance/nn, independence/nn, interest/nn, isolation/nn, leeway/nn, opportunity/nn, patronage/nn, peace/nn, policy/nn, potential/nn, power/nn, preference/nn, process/nn, preference/nn, push/nn, rein/nn, self-esteem/nn, skill/nn, strenght/nn, support/nn, talent/nn, thrust/nn, traction/nn, variance/nn\nTheme.drug: drug/nn, ache/nn, aids/nn, awareness/nn, blood/nn, diet/nn, effect/nn, evaluation/nn, exercise/nn, exposure/nn, hdls/np, headache/nn, injection/nn, interaction/nn, medication/nn, medicine/nn, penicillin/nn, pill/nn, prescription/nn, rash/nn, relaxation/nn, simulation/nn, stroke/nn, test/nn, testing/nn, therapy/nn, transfusion/nn, workout/nn\nTheme.doubt: doubt/nn, affection/nn, answer/nn, appreciation/nn, background/nn, experience/nn, feeling/nn, grief/nn, idea/nn, impression/nn, insight/nn, knowledge/nn, life/nn, motivation/nn, motive/nn, payback/nn, perspective/nn, question/nn, reason/nn, respect/nn, sense/nn, slant/nn, sympathy/nn, thought/nn, uplift/nn, view/nn\n\n\n\n\n\nQLVL colloquium - Leuven, 2022-11-4"
  },
  {
    "objectID": "QLVL_colloquium/index.html#parameters-of-the-type-level-models",
    "href": "QLVL_colloquium/index.html#parameters-of-the-type-level-models",
    "title": "Annotation goes distributional:",
    "section": "Parameters of the type-level models",
    "text": "Parameters of the type-level models\n\n\nCorpus filters: window size (3, 4, 7), sentence boundaries (y/n)\nCollocation matrix filters: selection of context words (dimensionality: 5000, 10000, 50000), vocabulary filtering (e.g., PoS, lemma, stop-words)\nAssociation measures: PPMI, log likelihood\nDistance measures: Cosine\nDimensionality reduction: Euclidian + UMAP (none, 2, 10)\nK-medoids (PAM): 3, 8, 15"
  },
  {
    "objectID": "Taal_tongval/abstract.html",
    "href": "Taal_tongval/abstract.html",
    "title": "Annotation goes distributional:",
    "section": "",
    "text": "[The waiter]subject [gave]verb [my cousin]recipient [some pizza]theme\n[The waiter]subject [gave]verb [some pizza]theme [to my cousin]recipient\n\nMany quantitative studies focused on the more traditional formal predictors, such as structural complexity of constituents, pronominality, constituent length (e.g. Szmrecsanyi et al 2017). However, little is known about the semantic characteristics of the lexical context in which those variants are embedded. The extensive variationist alternation research considers semantic predictors as mere “nuisance” factors at best, without much substantial interest in them or in their relative explanatory power.\nOne of the reasons behind this lack of interest is the labor‐intensive annotation process for semantic predictors. Röthlisberger et al. (2017) has tried to tackle this issue via inclusion of lexical randomeffects in regression analysis, but while this method works reasonably well to increase the goodness of fit of regression models, it does not explain much.\nIn this presentation, we attempt to analyze the influence that semantic differences in the meanings of slot fillers have on the choice between the two variants of the dative alternation by including clustering solutions based on distributional models (Lenci 2018) as predictors in regression models. The objectives of this research are to determine how important semantic properties of the lexical context are for predicting variant choice, and whether and what they add to traditional predictors’ explanatory power. To accomplish our aims, we restrict attention to FIne‐Grained Ambitious Semantic predictors (henceforth: FIGAS predictors), which are obtained in a bottom‐up fashion by using type-level vector space models. In particular, the heads of the noun phrases that take the role of theme and recipient (e.g. pizza and cousin, respectively, in (i)) are modeled as strings of numbers called vectors, which represent which context words each word is attracted to and how much. Their mathematical properties allow us to compute weighted distances between them and thus gauge how similar two words are based on which other words they are attracted to. As a consequence, we can cluster both the theme heads and the recipient heads and use the resulting classes as categorical predictors in the regression model.\nThe preliminary findings suggest that semantic clusters have significant predictive power, but traditional predictors appear to be subtly more powerful than type-level semantic predictors. Nevertheless, lexical effects emerge as the most important features for both theme and recipients.\n\n\n\n\n\n\n\nReferences\nBresnan, J., Cueni, A., Nikitina, T., Baayen, H., 2007. Predicting the Dative Alternation, in: Bouma, G., Kraemer, I., Zwarts, J. (Eds.), Cognitive Foundations of Interpretation. Royal Netherlands Academy of Science, Amsterdam, pp. 69–94.\nDavies, M., 2008-. The Corpus of Contemporary American English (COCA). Available online at https://www.englishcorpora.org/coca/. Lenci, A., 2018. Distributional models of word meaning. Annual review of Linguistics, 4, 151-171.\nLenci, A., 2018. Distributional Models of Word Meaning. Annual Review of Linguistics 4 (1): 151–71.\nRöthlisberger, M., Grafmiller, J., Szmrecsanyi, B., 2017. Cognitive indigenization effects in the English dative alternation. Cogn. Linguist. 28, 673–710.\nSzmrecsanyi, B., Grafmiller, J., Bresnan, J., Rosenbach, A., Tagliamonte, S., Todd, S., 2017. Spoken syntax in a comparative perspective: The dative and genitive alternation in varieties of English. Glossa J. Gen. Linguist. 2, 86."
  },
  {
    "objectID": "Taal_tongval/index.html#outline",
    "href": "Taal_tongval/index.html#outline",
    "title": "Annotation goes distributional:",
    "section": "Outline",
    "text": "Outline\n\nIntroduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nConclusions\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "Taal_tongval/index.html#the-dative-alternation",
    "href": "Taal_tongval/index.html#the-dative-alternation",
    "title": "Annotation goes distributional:",
    "section": "The dative alternation",
    "text": "The dative alternation\n\n(1) a. Ditransitive dative variant\n\n    [The waiter]subject [gave]verb [my cousin]recipient     [some pizza]theme\n\n   b. Prepositional dative variant\n\n    [The waiter]subject [gave]verb [some pizza]theme     [to my cousin]recipient\n\nLet’s dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as “two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker”.\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order)."
  },
  {
    "objectID": "Taal_tongval/index.html#modelling-grammatical-alternations",
    "href": "Taal_tongval/index.html#modelling-grammatical-alternations",
    "title": "Annotation goes distributional:",
    "section": "Modelling grammatical alternations",
    "text": "Modelling grammatical alternations\nIn previous literature, focus on three kinds of predictors:\n\nFormal predictors: e.g., structural complexity of constituents (e.g., presence of heavy postmodification), pronominality, and constituent length (in words, syllables, or similar)\nInformation status‐related predictors: e.g., givenness\nSemantic, coarse-grained, higher-level predictors: e.g., animacy (i.e., annotate for a binary distinction between animate and inanimate recipients – see Bresnan et al. 2007)\n\n\n\n\n\nTo explore the correlation of choices between the two variants both language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc) are implemented. Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal, top-down predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy, which can be defined as a top-down notion. This is because (next slide)"
  },
  {
    "objectID": "Taal_tongval/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "href": "Taal_tongval/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "title": "Annotation goes distributional:",
    "section": "Role of semantic predictors in alternation predictions",
    "text": "Role of semantic predictors in alternation predictions\nAnnotating for semantics is labor‐intensive and challenging to perform objectively.\n\nWhat role do semantic characteristics play in the choice of one of the two variants?\nAt the current state of the research, we know very little about them.\n\n\nWe assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.\n\n\nAnnotating for semantics is labor‐intensive and time-consuming, and it’s challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice."
  },
  {
    "objectID": "Taal_tongval/index.html#automatically-generated-semantic-predictors",
    "href": "Taal_tongval/index.html#automatically-generated-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "Automatically-generated semantic predictors",
    "text": "Automatically-generated semantic predictors\n\nWe use automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).\n\n\n\nNew inputs from DMSs: more data, less annotation\nDistributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.(Lenci 2018)\n\n\n\nWhat we propose is a completely bottom-up approach to annotation by using …. predictors.\nDSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from a corpus: those differences can tell us something about the semantic relatedness of items."
  },
  {
    "objectID": "Taal_tongval/index.html#examples-recipient-type-lemmas",
    "href": "Taal_tongval/index.html#examples-recipient-type-lemmas",
    "title": "Annotation goes distributional:",
    "section": "Examples: recipient type-lemmas",
    "text": "Examples: recipient type-lemmas\n\n(2) a. DAT-4100\n\n\n     [if I]subject [gave]verb [it]theme [to the government]recipient      they would just waste it.\n\n\n   b. DAT-4067\n\n\n     [The judge]subject [will usually, uh, give]verb [custody]theme      [to the mother]recipient ninety-seven percent of the time.\n\n\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation, here exemplified in two observations form our dataset. (read the examples)."
  },
  {
    "objectID": "Taal_tongval/index.html#matrix-of-recipients",
    "href": "Taal_tongval/index.html#matrix-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Matrix of recipients",
    "text": "Matrix of recipients\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n1\n16\n3\n0\n10\n\n\n\nmother/nn\n\n21\n0\n5\n12\n0\n\n\n\nadvance/nn\n\n0\n1\n0\n0\n5\n\n\n\n\n\nDative alternation dataset from Szmrecsanyi et al. (2017), which covers N = 1,190 dative observations in contemporary spoken American English (Switchboard corpus)\nCorpus for building DMSs: Corpus of Contemporary American English (COCA), spoken register (ca. 127 million tokens)\n\n\n\nIn this first part of the study, we implemented what we call a type-level model. -> what is a type?\nLet’s consider only the group of recipients, here exemplified by the types government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-word (from the dtasate) from the recipient slot, while each column represents a context word (from the corpus, with which we train the model): the aggregation of the frequencies between the single TW and all the CW constitutes a word-type vector, that is a distributional representation of a recipient lemma in that specific corpus. What you see here are raw frequencies, that is how many time the TW co-occur with the CW [example with gov]. However, ->"
  },
  {
    "objectID": "Taal_tongval/index.html#clouds-of-recipients",
    "href": "Taal_tongval/index.html#clouds-of-recipients",
    "title": "Annotation goes distributional:",
    "section": "Clouds of recipients",
    "text": "Clouds of recipients\n\nFigure 1: Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.\nGoing back to our question, what does it mean building the semantic predictors using DS?, it means clustering the type-word vectors, that is our recipient lemmas in this case, by first identifying the central member of the cluster (called medoid) from the data, and then grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, or 15 like in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "Taal_tongval/index.html#clouds-of-themes",
    "href": "Taal_tongval/index.html#clouds-of-themes",
    "title": "Annotation goes distributional:",
    "section": "Clouds of themes",
    "text": "Clouds of themes\n\nFigure 2: Scatterplot of themes lemmas. Colors are clusters, labelled by their central members."
  },
  {
    "objectID": "Taal_tongval/index.html#from-clouds-to-distributional-semantic-predictors",
    "href": "Taal_tongval/index.html#from-clouds-to-distributional-semantic-predictors",
    "title": "Annotation goes distributional:",
    "section": "From clouds to distributional semantic predictors",
    "text": "From clouds to distributional semantic predictors\n\nEach grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor.\n\n\nThe prediction of the dative variants is based on the membership of the recipient/theme type-lemma in a particular semantic cluster.\n\n\nTo summarize, each grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor. In what follows, we will use two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis, to predict dative choices based on the membership of the recipients/themes in a particular distributional cluster."
  },
  {
    "objectID": "Taal_tongval/index.html#random-forest",
    "href": "Taal_tongval/index.html#random-forest",
    "title": "Annotation goes distributional:",
    "section": "Random forest",
    "text": "Random forest\n\nRandom forest of traditional and distributional predictors\n\n\n\nCRF is a multivariate statistical method that can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "Taal_tongval/index.html#inside-the-most-important-clouds",
    "href": "Taal_tongval/index.html#inside-the-most-important-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the most important clouds",
    "text": "Inside the most important clouds\n\nTheme.one:\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\nit/pp, that/dd, theory/nn, thing/nn"
  },
  {
    "objectID": "Taal_tongval/index.html#another-look-at-the-clouds-regression-modelling",
    "href": "Taal_tongval/index.html#another-look-at-the-clouds-regression-modelling",
    "title": "Annotation goes distributional:",
    "section": "Another look at the clouds: regression modelling",
    "text": "Another look at the clouds: regression modelling\n\nc-values of regression model with only traditional and traditional+semantic predictors\n\nIndex of concordance (c-value) = non-parametric measure of how well a statistical model fits a set of observations.\n\n\n\n\n\n\n\n\n\n\n\nRM only trad predictors\nRM trad+sem predictors\n\n\n\n\nC-value for fixed effects\n0.984\n0.975\n\n\nC-value for fixed and random effects\n0.985\n0.993\n\n\n\n\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\nWe computed the Concordance index C: goodness of fit (read the slide) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the T-model suggests how the traditional p performs better alone, than in combination with the semantic ones."
  },
  {
    "objectID": "Taal_tongval/index.html#take-home-messages",
    "href": "Taal_tongval/index.html#take-home-messages",
    "title": "Annotation goes distributional:",
    "section": "Take-home messages",
    "text": "Take-home messages\n\n“You shall choose a variant by the company it keeps!”\n(semi-cit. Firth 1955)\n\n\nOverall, there are some distributional semantics clusters that are very predictive of the alternation\nStatistical analyses show how traditional predictors outperform distributional semantics predictors in terms of model performances\n\n\n\nbased on the c-value"
  },
  {
    "objectID": "Taal_tongval/index.html#directions-for-future-research",
    "href": "Taal_tongval/index.html#directions-for-future-research",
    "title": "Annotation goes distributional:",
    "section": "Directions for future research",
    "text": "Directions for future research\n\nOther alternations:\n\nClausal complementation alternation in the history of English\nProgressive alternation in Italian\n\nExperiment with token-level modeling\n\nsee Montes (2021)\n\n\n\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context"
  },
  {
    "objectID": "Taal_tongval/index.html#thoughts-comments-suggestions",
    "href": "Taal_tongval/index.html#thoughts-comments-suggestions",
    "title": "Annotation goes distributional:",
    "section": "Thoughts? Comments? Suggestions?",
    "text": "Thoughts? Comments? Suggestions?\n\n\nchiara.paolini@kuleuven.be"
  },
  {
    "objectID": "Taal_tongval/index.html#parameters-of-the-type-level-models",
    "href": "Taal_tongval/index.html#parameters-of-the-type-level-models",
    "title": "Annotation goes distributional:",
    "section": "Parameters of the type-level models",
    "text": "Parameters of the type-level models\n\n\nCorpus filters: window size (3, 4, 7), sentence boundaries (y/n)\nCollocation matrix filters: selection of context words (dimensionality: 5000, 10000, 50000), vocabulary filtering (e.g., PoS, lemma, stop-words)\nAssociation measures: PPMI, log likelihood\nDistance measures: Cosine\nDimensionality reduction: Euclidian + UMAP (none, 2, 10)\nK-medoids (PAM): 3, 8, 15"
  },
  {
    "objectID": "Taal_tongval/index.html#traditional-predictors",
    "href": "Taal_tongval/index.html#traditional-predictors",
    "title": "Annotation goes distributional:",
    "section": "Traditional predictors",
    "text": "Traditional predictors\n\n\nRecipient/Theme.type: The annotation distinguishes between the following categories: (1) noun phrase; (2) personal pronoun; (3) demonstrative pronoun; (4) impersonal pronoun.\nRecipient/Theme.definiteness: The annotation distinguishes between the following categories: (1) definite; (2) indefinite (3) definite proper noun.\nRecipient/Theme.animacy: The annotation distinguishes between the following categories: (1) human and animal; (2) collective; (3) temporal; (4) locative; (5) inanimate.\nLength.difference: The log difference between recipient and theme lengths (see Bresnan & Ford 2010).\nSemantics (of dative verb): (1) transfer; (2) communication; (3) abstract.\nRecipient/Theme.head: Head lexeme of both the theme and the recipient."
  },
  {
    "objectID": "Taal_tongval/index.html#inside-the-recipient-clouds",
    "href": "Taal_tongval/index.html#inside-the-recipient-clouds",
    "title": "Annotation goes distributional:",
    "section": "Inside the recipient clouds",
    "text": "Inside the recipient clouds\n\n\nRecipient.advance: advance/vv, america/np, arm/nn, country/nn, europe/np, government/nn, iranian/nn, nation/nn, nicaragua/np, russia/np, schwartzkopf/np, troop/nn, vietnamese/np\nRecipient.wife: actress/nn, wife/nn, artist/nn, boss/nn, brother/nn, daughters/nn, friend/nn, husband/nn, kitty/np, ryan/np, sister/nn, thomases/np, trek/np\nRecipient.kids: agencies/nn, boys/nn, businesses/nn, companies/nn, countries/nn, criminals/nn, dogs/nn, employees/nn, friends/nn, grandparents/nn, guys/nn, horses/nn, houses/nn, kids/nn, members/nn, parents/nn, rashad/np, sons/nn, students/nn, things/nn, twins/nn, women/nn\nRecipient.people: people/nn, citizen/nn, folk/nn, individual/nn, other/nn, peasant/nn, public/nn, reader/nn, white/nn\nRecipient.person: person/nn, convict/nn. criminal/nn, guy/nn, man/nn, someone/pn, stranger/nn\nRecipient.court: court/nn, judge/nn, jury/nn, matter/nn, plaintiff/nn, topic/nn\nRecipient.business: business/nn, car/nn, charity/nn, company/nn, competitor/nn, employee/nn, enterprise/nn, environment/nn, food/nn, grower/nn, household/nn, management/nn, taxpayer/nn"
  },
  {
    "objectID": "Taal_tongval/index.html#inside-the-theme-clouds-1",
    "href": "Taal_tongval/index.html#inside-the-theme-clouds-1",
    "title": "Annotation goes distributional:",
    "section": "Inside the theme clouds #1",
    "text": "Inside the theme clouds #1\n\n\nTheme.discount: discount/nn, allowance/nn, amount/nn, bargain/nn, benefit/nn, bill/nn, billion/nn, bond/nn, bonus/nn, buck/nn, budget/nn, card/nn, cash/nn, cent/nn, checkbook/nn, contract/nn, credit-card/nn, credit/nn, cut/nn, deal/nn, degree/nn, discount/nn, dollar/nn, estimate/nn, extra/nn, incentive/nn, income/nn, insurance/nn, less/da, limit/nn, loan/nn, mail/nn, million/nn, minimum/nn, money/nn, more/da, mortgage/nn, much/da, nickel/nn, number/nn, offer/nn, outlook/nn, pay/nn, paycheck/nn, payment/nn, penny/nn, percent/nn, physical/nn, point/nn, price/nn, profit-sharing/nn, raise/nn, rate/nn, rating/nn, rebate/nn, refund/nn, salary/nn, scale/nn, stipend/nn, stock/nn, tariff/nn, tax-credit/nn, tax/nn, thumbs-down/nn, ticket/nn, tune-up/nn, veto/nn, warranty/nn, worth/nn\nTheme.book: book/nn, address/nn, appearance/nn, assistant/nn, band/nn, booklet/nn, book/nn, break/nn, broadcast/nn, cable/nn, call/nn, comedy/nn, demo/nn, description/nn, entertainment/nn, f/np, figure/nn, game/nn, glimpse/nn, graph/nn, headline/nn, history/nn, husband/nn, list/nn, literature/nn, look/nn, magazine/nn, meeting/nn, message/nn, music/nn, name/nn, new/jj, news/nn, newspaper/nn, novel/nn, paper/nn, picture/nn, present/nn, rap/nn, report/nn, run/nn, rush/nn, screen/nn, sermon/nn, song/nn, speech/nn, story/nn, subscription/nn, subtitle/nn, synopsis/nn, talk/nn, tape/nn, television/nn, this/dd, tile/nn, topic/nn, trip/nn, tv/nn, update/nn, warning/nn, world/nn\nTheme.bag: bag/nn, amp/nn, arm/nn, azalea/nn, backpack/nn, ball/nn, bath/nn, bedroom/nn, bike/nn, bin/nn, body/nn, bulb/nn, can/nn, car/nn, coat/nn, deer/nn, dog/nn, glass/nn, gun/nn, inn/nn, junkyard/nn, knife/nn, light/nn, paint/nn, pair/nn, parking/nn, pt/nn, pipe/nn, plate/nn, pony/nn, puppy/nn, purse/nn, rabbit/nn, ride/nn, ring/nn, room/nn, seat/nn, shirts/nn, shirt/nn, shower/nn, spec/nn, spot/nn, suit/nn, sweatshirt/nn, tub/nn, uniform/nn, walking/nn"
  },
  {
    "objectID": "Taal_tongval/index.html#inside-the-theme-clouds-2",
    "href": "Taal_tongval/index.html#inside-the-theme-clouds-2",
    "title": "Annotation goes distributional:",
    "section": "Inside the theme clouds #2",
    "text": "Inside the theme clouds #2\n\n\nTheme.appeal: appeal/nn, appointment/nn, choice/nn, custody/nn, decision/nn, demonstration/nn, demonstrator/nn, detention/nn, election/nn, felony/nn, jurisdiction/nn, law/nn, nomination/nn, order/nn, parole/nn, penalty/nn, punishment/nn, release/nn, right/nn, rule/nn, sentence/nn, trial/nn, verdict/nn\nTheme.power: power/nn, ability/nn, activity/nn, advantage/nn, ambition/nn, attention/nn, balance/nn, chance/nn, command/nn, concession/nn, consistency/nn, control/nn, credibility/nn, dignity/nn, direction/nn, flexibility/nn, focus/nn, foothold/nn, freedom/nn, goal/nn, importance/nn, independence/nn, interest/nn, isolation/nn, leeway/nn, opportunity/nn, patronage/nn, peace/nn, policy/nn, potential/nn, power/nn, preference/nn, process/nn, preference/nn, push/nn, rein/nn, self-esteem/nn, skill/nn, strenght/nn, support/nn, talent/nn, thrust/nn, traction/nn, variance/nn\nTheme.drug: drug/nn, ache/nn, aids/nn, awareness/nn, blood/nn, diet/nn, effect/nn, evaluation/nn, exercise/nn, exposure/nn, hdls/np, headache/nn, injection/nn, interaction/nn, medication/nn, medicine/nn, penicillin/nn, pill/nn, prescription/nn, rash/nn, relaxation/nn, simulation/nn, stroke/nn, test/nn, testing/nn, therapy/nn, transfusion/nn, workout/nn\nTheme.doubt: doubt/nn, affection/nn, answer/nn, appreciation/nn, background/nn, experience/nn, feeling/nn, grief/nn, idea/nn, impression/nn, insight/nn, knowledge/nn, life/nn, motivation/nn, motive/nn, payback/nn, perspective/nn, question/nn, reason/nn, respect/nn, sense/nn, slant/nn, sympathy/nn, thought/nn, uplift/nn, view/nn\n\n\n\n\n\nTaal en Tongval Colloquium - Gent, 2022-11-18"
  },
  {
    "objectID": "Taal_tongval/index.html",
    "href": "Taal_tongval/index.html",
    "title": "Annotation goes distributional:",
    "section": "",
    "text": "Introduction: the dative alternation\nFrom vectors to predictors\nForests and clouds\nConclusions\n\n\nIn this talk, I would like to introduce you to the first case study of my PhD research project called “How much does meaning matter? A fresh look at grammatical alternations”. The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors."
  },
  {
    "objectID": "Taal_tongval/index.html#matrix-of-recipients---ppmi",
    "href": "Taal_tongval/index.html#matrix-of-recipients---ppmi",
    "title": "Annotation goes distributional:",
    "section": "Matrix of recipients - PPMI",
    "text": "Matrix of recipients - PPMI\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n0\n3.23\n0.21\n0\n2.59\n\n\n\nmother/nn\n\n4.36\n0\n1.65\n2.89\n0\n\n\n\nadvance/nn\n\n0\n2.09\n0\n0\n3.67\n\n\n\n\n\nPointwise Mutual Information (PMI): the (log) probability of co-occurrence scaled by the product of the single probability of occurrence. In a nutshell, we want to quantify the likelihood of co-occurrence of two words if the two of them were independent.\nPositive Pointwise mutual information (PPMI): Based on the same principle of the PMI, the negative values are replaced by 0 to better manage the possible inaccuracies that come from them.\n\n\n\nRaw frequencies are transformed, or better, weighted using association strength measures, such as PMI or PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\niIn other words, the values here represent the the attraction, or the association strength between the target word and a context word.\nIt is important to acknowledge that building a DS model means training a DS model workflow, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\nThere is a third matrix, namely distance matrix, computed to represent the distributional distance between the lemmas."
  },
  {
    "objectID": "MCL_seminar/study.html",
    "href": "MCL_seminar/study.html",
    "title": "Predicting the dative alternation in English",
    "section": "",
    "text": "Here is presented a step-by-step variation study on the English dative alternation based on Szmrecsanyi et al. (2017), using two traditional statistical techniques in variationist analysis: binomial logistic regression analysis and conditional random forest.\nFor the theoretical explanation of the dataset and the techniques, please check the slides."
  },
  {
    "objectID": "MCL_seminar/study.html#setup",
    "href": "MCL_seminar/study.html#setup",
    "title": "Predicting the dative alternation in English",
    "section": "Setup",
    "text": "Setup\nThe analysis needs the activation of different packages, lme4 and party for regression modeling with random effects and conditional random forest respectively. Hmisc, car and MuMIn are pivotal for calculating evaluation measures for regression modeling, while the library effects allows the plot of partial effects plot.\nMake sure to have them installed on your R studio version before running the script!\n\nlibrary(tidyverse)   # Easily Load the 'Tidyverse'\nlibrary(here)        # Enable easy file referencing in project-oriented workflows\nlibrary(lme4)        # for mixed-effects regression\nlibrary(Hmisc)       # to calculate C values\nlibrary(car)         # to calculate VIFs\nlibrary(MuMIn)       # Multi-Model Inference for PSeudo R2 measures\nlibrary(effects)     # for partial effects plot\nlibrary(party)       # for ctrees and CRF\nlibrary(kableExtra)  # Construct Complex Table with 'kable' and Pipe Syntax\n\nUpload and filter the dataset\nThe dataset I will provide has already been manipulated: it represents only the American subsection of the original dataset (1190 observations), plus it contains recipient/theme.lemma.\n\ndat_us <- read_tsv(here::here('MCL_seminar', 'data', 'dat_us.tsv'), show_col_types = F)\nhead(dat_us)\n\n# A tibble: 6 × 42\n  Token…¹ Variety Corpus Speaker Speak…² Speak…³ Context Inclu…⁴ Inclu…⁵ Seman…⁶\n  <chr>   <chr>   <chr>    <dbl> <chr>     <dbl> <chr>   <chr>   <lgl>   <chr>  \n1 dat-26… US      SWBD      1002 F          1963 A: Rig… y       NA      T      \n2 dat-26… US      SWBD      1016 F          1945 being … y       NA      T      \n3 dat-26… US      SWBD      1026 F          1957 of tha… y       NA      A      \n4 dat-26… US      SWBD      1031 F          1940 but it… y       NA      A      \n5 dat-26… US      SWBD      1033 F          1965 B: Yea… y       NA      T      \n6 dat-26… US      SWBD      1050 M          1950 B: -- … y       NA      A      \n# … with 32 more variables: Response.variable <chr>, Verb.form <chr>,\n#   Verb.particle <lgl>, Recipient <chr>, Recipient.head <chr>,\n#   Recipient.type <chr>, Recipient.animacy <chr>,\n#   Recipient.definiteness <chr>, Recipient.length <dbl>,\n#   Recipient.coordinated <dbl>, Recipient.quantified.pronoun <dbl>,\n#   Recipient.contains.contraction <dbl>, Recipient.contains.acronym <dbl>,\n#   Recipient.contains.multiword.number <dbl>, …\n\n\nAfter uploading the dataset, the second step is to filter and model the dataset based on our research needs. Specifically, here I first select only the predictors we are interested into and filter out the observations with any NA values in the predictors and the ones that do not contain the value F or M in the Speaker.sex predictor. Then, I proceed to create the binary predictors as described in the paper, and the new Length.difference variable by calculating the difference between the log values of Theme/Recipient.length. I also did here some level adjustment for the regression analysis: I set the reference level for the Response.variable as Prepositional, thus the predicted odds will be for the Ditransitive variant. Speaker.pruned and Theme/Recipient.lemma.pruned are pruned variables to be used in the regression model as random effects (I will get into details in the regression section): fct_lump_min creates a new level called other for lemmas that appear fewer than 2 times. Eventually, the character variables should be mutated in factors.\n\n\n\n\n\n\nFactors are used to represent categorical data in R.\n\n\n\nFactors are stored as integers, and have labels associated with these unique integers (levels). While factors look (and often behave) like character vectors, they are actually integers under the hood, and you need to be careful when treating them like strings.\nOnce created them using the factor() command, factors can only contain a pre-defined set values, known as levels. By default, R always sorts levels in alphabetical order.\n\n\n\ndat_us <- dat_us %>% \n  select(Token.ID, Variety, Speaker, Speaker.sex, Response.variable,\n         Recipient.type, Theme.type,\n         Recipient.definiteness, Theme.definiteness,\n         Recipient.animacy, Theme.animacy,\n         Recipient.length, Theme.length,\n         Semantics,\n         Recipient.head, Theme.head, Recipient.lemma, Theme.lemma) %>% \n  na.omit() %>%\n  filter(Speaker.sex %in% c('F', 'M')) %>%\n  mutate(\n    Recipient.type.bin = if_else(Recipient.type == 'N', 'N', 'P'),\n    Theme.type.bin = if_else(Theme.type == 'N', 'N', 'P'),\n    Recipient.definiteness.bin = if_else(Recipient.definiteness == 'Indefinite', 'Indefinite', 'Definite'),\n    Theme.definiteness.bin = if_else(Theme.definiteness == 'Indefinite', 'Indefinite', 'Definite'),\n    Recipient.animacy.bin = if_else(Recipient.animacy == 'A', 'A', 'I'),\n    Theme.animacy.bin = if_else(Theme.animacy == 'A', 'A', 'I'),\n    Length.difference = log(Recipient.length) - log(Theme.length)) %>% \n  mutate(across(where(is.character), as.factor), \n    Speaker = factor(Speaker),\n    Response.variable = fct_relevel(Response.variable, \"P\"), #level the response variable: reference level is P-dative, thus predicted odds are for the D-dative\n    Speaker.pruned = fct_lump_min(Speaker, 5, other_level = \"other\"), #pruning the random effects\n    Theme.lemma.pruned = fct_lump_min(Theme.lemma, 2, other_level = \"other\"),\n    Recipient.lemma.pruned = fct_lump_min(Recipient.lemma, 2, other_level = \"other\")) \n head(dat_us)%>%\n  kbl(fixed_thead = T) %>%\n  kable_paper()\n\n\n\n\n Token.ID \n    Variety \n    Speaker \n    Speaker.sex \n    Response.variable \n    Recipient.type \n    Theme.type \n    Recipient.definiteness \n    Theme.definiteness \n    Recipient.animacy \n    Theme.animacy \n    Recipient.length \n    Theme.length \n    Semantics \n    Recipient.head \n    Theme.head \n    Recipient.lemma \n    Theme.lemma \n    Recipient.type.bin \n    Theme.type.bin \n    Recipient.definiteness.bin \n    Theme.definiteness.bin \n    Recipient.animacy.bin \n    Theme.animacy.bin \n    Length.difference \n    Speaker.pruned \n    Theme.lemma.pruned \n    Recipient.lemma.pruned \n  \n\n\n dat-2626 \n    US \n    1002 \n    F \n    D \n    P \n    I \n    Definite \n    Indefinite \n    A \n    I \n    1 \n    1 \n    T \n    us \n    something \n    us/pp \n    something/pn \n    P \n    P \n    Definite \n    Indefinite \n    A \n    I \n    0 \n    1002 \n    something/pn \n    us/pp \n  \n\n dat-2627 \n    US \n    1016 \n    F \n    D \n    P \n    P \n    Definite \n    Definite \n    A \n    I \n    1 \n    1 \n    T \n    me \n    hers \n    me/pp \n    hers/pp \n    P \n    P \n    Definite \n    Definite \n    A \n    I \n    0 \n    1016 \n    other \n    me/pp \n  \n\n dat-2628 \n    US \n    1026 \n    F \n    D \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    1 \n    1 \n    A \n    her \n    options \n    her/pp \n    option/nn \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    0 \n    other \n    option/nn \n    her/pp \n  \n\n dat-2629 \n    US \n    1031 \n    F \n    D \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    1 \n    1 \n    A \n    us \n    demos \n    us/pp \n    demos/nn \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    0 \n    other \n    other \n    us/pp \n  \n\n dat-2630 \n    US \n    1033 \n    F \n    D \n    P \n    I \n    Definite \n    Indefinite \n    A \n    I \n    1 \n    1 \n    T \n    us \n    one \n    us/pp \n    one/pn \n    P \n    P \n    Definite \n    Indefinite \n    A \n    I \n    0 \n    1033 \n    one/pn \n    us/pp \n  \n\n dat-2631 \n    US \n    1050 \n    M \n    D \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    1 \n    1 \n    A \n    me \n    shelter \n    me/pp \n    shelter/nn \n    P \n    N \n    Definite \n    Indefinite \n    A \n    I \n    0 \n    other \n    shelter/nn \n    me/pp"
  },
  {
    "objectID": "MCL_seminar/index.html#outline",
    "href": "MCL_seminar/index.html#outline",
    "title": "Predicting the dative alternation in English:",
    "section": "Outline",
    "text": "Outline\n\nPreliminary steps\nThe case study\n\n\nStep-by-step case study (partially) based on Szmrecsanyi et al. (2017). We will investigate the dative alternation in American English by employing two of the traditional statistical techniques in variationist analysis: *binomial logistic regression analysis and conditional random forest*."
  },
  {
    "objectID": "MCL_seminar/index.html#the-dative-alternation",
    "href": "MCL_seminar/index.html#the-dative-alternation",
    "title": "Predicting the dative alternation in English:",
    "section": "The dative alternation",
    "text": "The dative alternation\n\n(1) a. Ditransitive dative variant\n\n    [The waiter]subject [gave]verb [my cousin]recipient     [some pizza]theme\n\n   b. Prepositional dative variant\n\n    [The waiter]subject [gave]verb [some pizza]theme     [to my cousin]recipient\n\nWhat is an alternation? See Pijpops (2020) and Gries (2017).\n\n\nLet’s dive into our case study.\nThe topic of our analysis is the dative alternation in *contemporary American* English.\nThe DA is one of the most investigated cases of grammatical alternation - where we define a SA as “two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker”.\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order)."
  },
  {
    "objectID": "MCL_seminar/index.html#modelling-grammatical-alternations",
    "href": "MCL_seminar/index.html#modelling-grammatical-alternations",
    "title": "Predicting the dative alternation in English:",
    "section": "Modelling grammatical alternations",
    "text": "Modelling grammatical alternations\nIn previous literature, focus on three kinds of predictors:\n\nFormal predictors: e.g., structural complexity of constituents (e.g., presence of heavy postmodification), pronominality, and constituent length (in words, syllables, or similar)\nInformation status‐related predictors: e.g., givenness\nSemantic, coarse-grained, higher-level predictors: e.g., animacy (i.e., annotate for a binary distinction between animate and inanimate recipients – see Bresnan et al. 2007)\n\n\n\n\n\nTo explore the correlation of choices between the two variants both language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc) are implemented. Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal, top-down predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy, which can be defined as a top-down notion. This is because (next slide)"
  },
  {
    "objectID": "MCL_seminar/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "href": "MCL_seminar/index.html#role-of-semantic-predictors-in-alternation-predictions",
    "title": "Predicting the dative alternation in English:",
    "section": "Role of semantic predictors in alternation predictions",
    "text": "Role of semantic predictors in alternation predictions\nAnnotating for semantics is labor‐intensive and challenging to perform objectively.\n\nWhat role do semantic characteristics play in the choice of one of the two variants?\nAt the current state of the research, we know very little about them.\n\n\nWe assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.\n\n\nAnnotating for semantics is labor‐intensive and time-consuming, and it’s challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice."
  },
  {
    "objectID": "MCL_seminar/index.html#automatically-generated-semantic-predictors",
    "href": "MCL_seminar/index.html#automatically-generated-semantic-predictors",
    "title": "Predicting the dative alternation in English:",
    "section": "Automatically-generated semantic predictors",
    "text": "Automatically-generated semantic predictors\n\nWe use automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).\n\n\n\nNew inputs from DMSs: more data, less annotation\nDistributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.(Lenci 2018)\n\n\n\nWhat we propose is a completely bottom-up approach to annotation by using …. predictors.\nDSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from a corpus: those differences can tell us something about the semantic relatedness of items."
  },
  {
    "objectID": "MCL_seminar/index.html#examples-recipient-type-lemmas",
    "href": "MCL_seminar/index.html#examples-recipient-type-lemmas",
    "title": "Predicting the dative alternation in English:",
    "section": "Examples: recipient type-lemmas",
    "text": "Examples: recipient type-lemmas\n\n(2) a. DAT-4100\n\n\n     [if I]subject [gave]verb [it]theme [to the government]recipient      they would just waste it.\n\n\n   b. DAT-4067\n\n\n     [The judge]subject [will usually, uh, give]verb [custody]theme      [to the mother]recipient ninety-seven percent of the time.\n\n\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation, here exemplified in two observations form our dataset. (read the examples)."
  },
  {
    "objectID": "MCL_seminar/index.html#matrix-of-recipients",
    "href": "MCL_seminar/index.html#matrix-of-recipients",
    "title": "Predicting the dative alternation in English:",
    "section": "Matrix of recipients",
    "text": "Matrix of recipients\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n1\n16\n3\n0\n10\n\n\n\nmother/nn\n\n21\n0\n5\n12\n0\n\n\n\nadvance/nn\n\n0\n1\n0\n0\n5\n\n\n\n\n\nDative alternation dataset from Szmrecsanyi et al. (2017), which covers N = 1,190 dative observations in contemporary spoken American English (Switchboard corpus)\nCorpus for building DMSs: Corpus of Contemporary American English (COCA), spoken register (ca. 127 million tokens)\n\n\n\nIn this first part of the study, we implemented what we call a type-level model. -> what is a type?\nLet’s consider only the group of recipients, here exemplified by the types government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-word (from the dtasate) from the recipient slot, while each column represents a context word (from the corpus, with which we train the model): the aggregation of the frequencies between the single TW and all the CW constitutes a word-type vector, that is a distributional representation of a recipient lemma in that specific corpus. What you see here are raw frequencies, that is how many time the TW co-occur with the CW [example with gov]. However, ->"
  },
  {
    "objectID": "MCL_seminar/index.html#matrix-of-recipients---ppmi",
    "href": "MCL_seminar/index.html#matrix-of-recipients---ppmi",
    "title": "Predicting the dative alternation in English:",
    "section": "Matrix of recipients - PPMI",
    "text": "Matrix of recipients - PPMI\n\n\n\n\n\n\n\n\n\n\n\n\n\ndaughter/nn\n\n\neurope/np\n\n\nit/pp\n\n\ndad/nn\n\n\ntroop/nn\n\n\n\n\n\n\ngovernment/nn\n\n0\n3.23\n0.21\n0\n2.59\n\n\n\nmother/nn\n\n4.36\n0\n1.65\n2.89\n0\n\n\n\nadvance/nn\n\n0\n2.09\n0\n0\n3.67\n\n\n\n\n\nPointwise Mutual Information (PMI): the (log) probability of co-occurrence scaled by the product of the single probability of occurrence. In a nutshell, we want to quantify the likelihood of co-occurrence of two words if the two of them were independent.\nPositive Pointwise mutual information (PPMI): Based on the same principle of the PMI, the negative values are replaced by 0 to better manage the possible inaccuracies that come from them.\n\n\n\nRaw frequencies are transformed, or better, weighted using association strength measures, such as PMI or PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\niIn other words, the values here represent the the attraction, or the association strength between the target word and a context word.\nIt is important to acknowledge that building a DS model means training a DS model workflow, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\nThere is a third matrix, namely distance matrix, computed to represent the distributional distance between the lemmas."
  },
  {
    "objectID": "MCL_seminar/index.html#clouds-of-recipients",
    "href": "MCL_seminar/index.html#clouds-of-recipients",
    "title": "Predicting the dative alternation in English:",
    "section": "Clouds of recipients",
    "text": "Clouds of recipients\n\nGoing back to our question, what does it mean building the semantic predictors using DS?, it means clustering the type-word vectors, that is our recipient lemmas in this case, by first identifying the central member of the cluster (called medoid) from the data, and then grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, or 15 like in our case.\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n(describe the clusters)"
  },
  {
    "objectID": "MCL_seminar/index.html#clouds-of-themes",
    "href": "MCL_seminar/index.html#clouds-of-themes",
    "title": "Predicting the dative alternation in English:",
    "section": "Clouds of themes",
    "text": "Clouds of themes"
  },
  {
    "objectID": "MCL_seminar/index.html#from-clouds-to-distributional-semantic-predictors",
    "href": "MCL_seminar/index.html#from-clouds-to-distributional-semantic-predictors",
    "title": "Predicting the dative alternation in English:",
    "section": "From clouds to distributional semantic predictors",
    "text": "From clouds to distributional semantic predictors\n\nEach grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor.\n\n\nThe prediction of the dative variants is based on the membership of the recipient/theme type-lemma in a particular semantic cluster.\n\n\nTo summarize, each grouping of recipient/theme lemmas represents what we call distributional (semantic) predictor. In what follows, we will use two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis, to predict dative choices based on the membership of the recipients/themes in a particular distributional cluster."
  },
  {
    "objectID": "MCL_seminar/index.html#random-forest",
    "href": "MCL_seminar/index.html#random-forest",
    "title": "Predicting the dative alternation in English:",
    "section": "Random forest",
    "text": "Random forest\n\nRandom forest of traditional and distributional predictors\n\n\nCRF is a multivariate statistical method that can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality"
  },
  {
    "objectID": "MCL_seminar/index.html#inside-the-most-important-clouds",
    "href": "MCL_seminar/index.html#inside-the-most-important-clouds",
    "title": "Predicting the dative alternation in English:",
    "section": "Inside the most important clouds",
    "text": "Inside the most important clouds\n\nTheme.one:\n\n\n\none/pn, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n\n\n\nRecipient.everybody:\n\n\n\neverybody/pn, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n\n\n\nRecipient.it:\n\n\n\nit/pp, that/dd, theory/nn, thing/nn"
  },
  {
    "objectID": "MCL_seminar/index.html#another-look-at-the-clouds-regression-modelling",
    "href": "MCL_seminar/index.html#another-look-at-the-clouds-regression-modelling",
    "title": "Predicting the dative alternation in English:",
    "section": "Another look at the clouds: regression modelling",
    "text": "Another look at the clouds: regression modelling\n\nc-values of regression model with only traditional and traditional+semantic predictors\n\nIndex of concordance (c-value) = non-parametric measure of how well a statistical model fits a set of observations.\n\n\n\n\n\n\n\n\n\n\n\nRM only trad predictors\nRM trad+sem predictors\n\n\n\n\nC-value for fixed effects\n0.984\n0.975\n\n\nC-value for fixed and random effects\n0.985\n0.993\n\n\n\n\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\nWe computed the Concordance index C: goodness of fit (read the slide) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the T-model suggests how the traditional p performs better alone, than in combination with the semantic ones."
  },
  {
    "objectID": "MCL_seminar/index.html#take-home-messages",
    "href": "MCL_seminar/index.html#take-home-messages",
    "title": "Predicting the dative alternation in English:",
    "section": "Take-home messages",
    "text": "Take-home messages\n\n“You shall choose a variant by the company it keeps!”\n(semi-cit. Firth 1955)\n\n\nOverall, there are some distributional semantics clusters that are very predictive of the alternation\nStatistical analyses show how traditional predictors outperform distributional semantics predictors in terms of model performances\n\n\n\nbased on the c-value"
  },
  {
    "objectID": "MCL_seminar/index.html#directions-for-future-research",
    "href": "MCL_seminar/index.html#directions-for-future-research",
    "title": "Predicting the dative alternation in English:",
    "section": "Directions for future research",
    "text": "Directions for future research\n\nOther alternations:\n\nClausal complementation alternation in the history of English\nProgressive alternation in Italian\n\nExperiment with token-level modeling\n\nsee Montes (2021)\n\n\n\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context\n\n\n\n\nMethods is Corpus Linguistics - Leuven, 2022-12-8"
  },
  {
    "objectID": "MCL_seminar/study.html#the-birds-eye-perspective-conditional-random-forest",
    "href": "MCL_seminar/study.html#the-birds-eye-perspective-conditional-random-forest",
    "title": "Predicting the dative alternation in English",
    "section": "The bird’s eye perspective: conditional random forest",
    "text": "The bird’s eye perspective: conditional random forest\nConditional random forest (CRF) is a multivariate statistical method that can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? For an accessible introduction to conditional inference trees and random forest, check out Levshina (2020).\nCRF can be done using a great range of libraries (partykit, ranger among them): here I use party, the classic library for trees and forests in R.\nSince it is a partitioning algorithm with random sampling, we need to set first a seed to get always the same result. forest represents the implementation of the formula for the CRF using the cforest() function: similarly to the formula for regression models, the syntax is: cforest(response variable ~ [variables], data = data_name) .\nAfter computing the CRF, varimp() computes variable importance measures: if conditional = TRUE, the importance of each variable is computed by adjusting for correlations between predictor variables.\nThe third chunk of the code is dedicated to the computation of the Concordance index C-value, better known as simply C-value, a non-parametric measure of how well a statistical model fits a set of observations. {Hmsic} is the reference library, using the function somers2().\n\n\n\n\n\n\nImportant\n\n\n\nC-value is a pivotal measure for most part of the statistical tools employed in variationist analysis, especially for regression modeling. However, make sure to not rely only on this measure for your assessments as it gives a general evaluation of the model. For CRF is fine, but we will see that it is not the case for regression modeling.\n\n\nFinally, it is possible to plot the result of the varimp computation and check for the most important variables in the prediction of the linguistic variants. The red dotted line represents the threshold to consider a variable slightly significant. For a detailed theoretical explanation, see Tagliamonte & Baayen (2012).\n\nset.seed(123)\nforest = cforest(Response.variable ~ \n              Speaker.sex +\n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin+\n              Recipient.animacy.bin+\n              Theme.animacy.bin+\n              Length.difference,\n              data = dat_us)\n\n#### variable importance ranking, takes some time\nforest.varimp = varimp(forest, conditional = TRUE) \n\n#### model C index\n#### C ranges between 0 an 1; the closer to 1, the better the model\nprob2.rf <- unlist(treeresponse(forest))[c(FALSE, TRUE)]\nsomerssmallcrf <- somers2(prob2.rf, as.numeric(dat_us$Response.variable) - 1)\nsomerssmallcrf[\"C\"]\n\n       C \n0.986687 \n\n### the following code creates a dot plot visualizing the variable importance ranking\nggplot(enframe(forest.varimp, name=\"Predictor\", value = \"Varimp\"),\n        aes(x = Varimp, y = reorder(Predictor, Varimp))) +\n    geom_point(size = 3, color = \"darkblue\") +\n    labs(x = \"Variable importance\", y = \"Predictor\") +\n    geom_vline(xintercept = abs(min(forest.varimp)),\n               color = \"red\", linetype = 2) +\n    theme_minimal(base_size = 20) +\n    theme(legend.position = c(0.9, 0.2))"
  },
  {
    "objectID": "MCL_seminar/study.html#the-jewelers-eye-perspective-regression-modeling",
    "href": "MCL_seminar/study.html#the-jewelers-eye-perspective-regression-modeling",
    "title": "Predicting the dative alternation in English",
    "section": "The jeweler’s eye perspective: regression modeling",
    "text": "The jeweler’s eye perspective: regression modeling\nTo take a closer look at the predictors, and their direction in predicting one of the two linguistic variants, we use binary logistic regression analysis with mixed effects as implemented in the lme4 package in R.\nRegression modeling has in general a very simple and standardize code, but it can be done in many different ways, using different techniques to get to the final results. Here I will show you the simplest bottom-up technique to get from a maximal model with all the predictors to a minimal model comprising of only the most meaningful predictors. The idea is to manually remove the predictor with the highest p-value at each run of the regression model, till we get a model with few meaningful predictors.\n\n\n\n\n\n\nNote\n\n\n\nThere are many ways to implement automatic algorithms to get minimal models by using stepwise modeling. However, I would recommend to use those automatic techniques when you feel to master an advance knowledge of your data and of regression modeling.\n\n\nMaximal regression model\nThe reference level is Prepositional, thus the predicted odds are for the Ditransitive alternation.\nglmer() is the function to use for regression models with mixed-effects. See the documentation for the formula and an in-depth explanation of the code.\n\n\n\n\n\n\nTip\n\n\n\nHere the monthly-updated GLMM bible for the thousands of problems in fitting the regression model.\n\n\n\ntrad_model <- glmer(Response.variable ~ \n              Speaker.sex +\n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin+\n              Recipient.animacy.bin+\n              Theme.animacy.bin+\n              Length.difference+\n              (1|Speaker)+ # random effect (intercept adjustment)\n              (1|Recipient.lemma)+\n              (1|Theme.lemma),\n              data = dat_us,\n              family=binomial\n)\n\nWarning in (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, :\nfailure to converge in 10000 evaluations\n\n\nWarning in optwrap(optimizer, devfun, start, rho$lower, control = control, :\nconvergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n\nsummary(trad_model)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Response.variable ~ Speaker.sex + Semantics + Recipient.type.bin +  \n    Theme.type.bin + Recipient.definiteness.bin + Theme.definiteness.bin +  \n    Recipient.animacy.bin + Theme.animacy.bin + Length.difference +  \n    (1 | Speaker) + (1 | Recipient.lemma) + (1 | Theme.lemma)\n   Data: dat_us\n\n     AIC      BIC   logLik deviance df.resid \n   279.6    350.5   -125.8    251.6     1156 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.8673  0.0036  0.0122  0.0383  3.4311 \n\nRandom effects:\n Groups          Name        Variance  Std.Dev.\n Theme.lemma     (Intercept) 8.9752300 2.99587 \n Speaker         (Intercept) 0.0001514 0.01231 \n Recipient.lemma (Intercept) 0.2152492 0.46395 \nNumber of obs: 1170, groups:  \nTheme.lemma, 478; Speaker, 345; Recipient.lemma, 130\n\nFixed effects:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            0.7024     2.0973   0.335 0.737696    \nSpeaker.sexM                          -0.2330     0.5548  -0.420 0.674556    \nSemanticsC                            -1.2814     1.0826  -1.184 0.236554    \nSemanticsT                            -1.4212     0.7170  -1.982 0.047479 *  \nRecipient.type.binP                    2.8195     0.6920   4.074 4.61e-05 ***\nTheme.type.binP                       -1.0570     1.1745  -0.900 0.368147    \nRecipient.definiteness.binIndefinite  -2.4946     1.0636  -2.345 0.019006 *  \nTheme.definiteness.binIndefinite       3.1257     0.9373   3.335 0.000853 ***\nRecipient.animacy.binI                -3.4990     1.0314  -3.392 0.000693 ***\nTheme.animacy.binI                     0.4650     2.0049   0.232 0.816591    \nLength.difference                     -2.6538     0.6783  -3.912 9.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Spkr.M SmntcC SmntcT Rcp..P Thm..P Rcpnt.d.I Thm.d.I\nSpeaker.sxM -0.186                                                     \nSemanticsC  -0.099  0.045                                              \nSemanticsT  -0.268  0.204  0.298                                       \nRcpnt.typ.P -0.122  0.116 -0.135 -0.188                                \nThm.typ.bnP -0.146  0.155  0.084 -0.030 -0.004                         \nRcpnt.dfn.I -0.103  0.142  0.122  0.217  0.062  0.100                  \nThm.dfntn.I -0.104  0.044 -0.032 -0.224  0.261  0.066 -0.503           \nRcpnt.nmc.I -0.060  0.044  0.119  0.316 -0.191  0.051  0.552    -0.662 \nThm.nmcy.bI -0.877 -0.070 -0.018  0.089  0.015  0.006 -0.130     0.087 \nLngth.dffrn -0.018  0.035  0.094  0.243  0.027 -0.130  0.530    -0.665 \n            Rcpnt.n.I Thm.n.I\nSpeaker.sxM                  \nSemanticsC                   \nSemanticsT                   \nRcpnt.typ.P                  \nThm.typ.bnP                  \nRcpnt.dfn.I                  \nThm.dfntn.I                  \nRcpnt.nmc.I                  \nThm.nmcy.bI -0.149           \nLngth.dffrn  0.640    -0.083 \noptimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\nfailure to converge in 10000 evaluations\n\n\nModel summaries: Pseudo R2 measures (coefficient of determination), C value, and ViF\nIn order to evaluate the performance of the model, it is possible to compute different measures:\n\nC-value (see section on CRF)\nPseudo-R squared: a goodness of fit measure explaining the improvement in model likelihood over a null model (see for discussion: Hemmert et al. (2018)). Here I compute the marginal pseudo-R2 (the variance explained by fixed factors) and the conditional pseudo-R2 (variance explained by both fixed and random factors (i.e. the entire model)).\nVariance Inflation Factors: A variance inflation factor (VIF) detects multicollinearity in regression analysis. Multicollinearity is when there is correlation between predictors (i.e. independent variables) in a model; the presence of multicollinearity can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model. A goof VIF should be lower than 2.5, but there is a lot of debate.\n\n\n# R2\nr.squaredGLMM(trad_model)\n\nWarning: 'r.squaredGLMM' now calculates a revised statistic. See the help page.\n\n\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n                     R2m          R2c\ntheoretical 5.989478e-01 8.942824e-01\ndelta       4.138664e-15 6.179394e-15\n\n# Concordance index C\nsomers2(binomial()$linkinv(fitted(trad_model)), as.numeric(dat_us$Response.variable) -1)\n\n           C          Dxy            n      Missing \n   0.9967788    0.9935576 1170.0000000    0.0000000 \n\n# Variance Inflation Factors\nvif(trad_model)\n\n                               GVIF Df GVIF^(1/(2*Df))\nSpeaker.sex                1.131815  1        1.063868\nSemantics                  1.298577  2        1.067498\nRecipient.type.bin         1.313491  1        1.146076\nTheme.type.bin             1.118899  1        1.057780\nRecipient.definiteness.bin 1.743511  1        1.320421\nTheme.definiteness.bin     2.526215  1        1.589407\nRecipient.animacy.bin      2.404110  1        1.550519\nTheme.animacy.bin          1.063911  1        1.031460\nLength.difference          2.439304  1        1.561827\n\n\nMinimal regression model\nAfter manually removing our not-significant predictors, we should get our minimal adequate regression model.\n\nFor the sake of length, I did not show every passage (I will show it in class).\n\nBefore starting pruning the model, a good practice is to improve the regression model in two steps:\n\nBy pruning the random effects (i.e. consider only the levels higher that a certain threshold);\nBy optimizing the model, using different techniques: one of the most common is the optimizer bobyqa which enhances the performance of the model together with optCtrl = list(maxfun = 100000) which allows the model to perform more runs.\n\n\ntrad_model_min <- glmer(Response.variable ~ \n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin +\n              Recipient.animacy.bin+\n              Length.difference+\n              (1|Speaker.pruned)+ # random effect (intercept adjustment)\n              (1|Recipient.lemma.pruned)+\n              (1|Theme.lemma.pruned),\n              data = dat_us,\n              family=binomial,\n              glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 100000))\n)\nsummary(trad_model_min)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Response.variable ~ Semantics + Recipient.type.bin + Theme.type.bin +  \n    Recipient.definiteness.bin + Theme.definiteness.bin + Recipient.animacy.bin +  \n    Length.difference + (1 | Speaker.pruned) + (1 | Recipient.lemma.pruned) +  \n    (1 | Theme.lemma.pruned)\n   Data: dat_us\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e+05))\n\n     AIC      BIC   logLik deviance df.resid \n   283.9    344.7   -129.9    259.9     1158 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-29.3657   0.0115   0.0291   0.0750   3.2863 \n\nRandom effects:\n Groups                 Name        Variance Std.Dev.\n Theme.lemma.pruned     (Intercept) 4.05088  2.0127  \n Speaker.pruned         (Intercept) 0.47153  0.6867  \n Recipient.lemma.pruned (Intercept) 0.06079  0.2465  \nNumber of obs: 1170, groups:  \nTheme.lemma.pruned, 158; Speaker.pruned, 89; Recipient.lemma.pruned, 38\n\nFixed effects:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            0.1945     0.7095   0.274   0.7840    \nSemanticsC                            -1.0726     0.7750  -1.384   0.1664    \nSemanticsT                            -1.0996     0.5448  -2.018   0.0436 *  \nRecipient.type.binP                    2.8374     0.6030   4.705 2.54e-06 ***\nTheme.type.binP                       -0.9247     0.8281  -1.117   0.2642    \nRecipient.definiteness.binIndefinite  -1.5220     0.6707  -2.269   0.0233 *  \nTheme.definiteness.binIndefinite       2.6062     0.5973   4.363 1.28e-05 ***\nRecipient.animacy.binI                -2.5117     0.5917  -4.245 2.19e-05 ***\nLength.difference                     -1.9668     0.3820  -5.148 2.63e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) SmntcC SmntcT Rcp..P Thm..P Rcpnt.d.I Thm..I Rcpnt.n.I\nSemanticsC  -0.198                                                       \nSemanticsT  -0.217  0.318                                                \nRcpnt.typ.P -0.352 -0.130 -0.105                                         \nThm.typ.bnP -0.204  0.078 -0.091 -0.051                                  \nRcpnt.dfn.I -0.262  0.067  0.135  0.210  0.059                           \nThm.dfntn.I -0.463  0.039 -0.106  0.201  0.105 -0.277                    \nRcpnt.nmc.I -0.108  0.078  0.233 -0.134  0.015  0.266    -0.452          \nLngth.dffrn  0.192  0.041  0.041  0.146 -0.202  0.245    -0.426  0.315   \n\n\nModel summaries: Pseudo R2 measures (coefficient of determination), C value and VIF\n\n# Pseudo R2 measures\nr.squaredGLMM(trad_model_min)\n\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n\n\n                  R2m       R2c\ntheoretical 0.5980292 0.8320310\ndelta       0.2696375 0.3751436\n\n# Concordance index C\nsomers2(binomial()$linkinv(fitted(trad_model_min)), as.numeric(dat_us$Response.variable) -1)\n\n           C          Dxy            n      Missing \n   0.9898890    0.9797781 1170.0000000    0.0000000 \n\n# Variance Inflation Factors\nvif(trad_model_min)\n\n                               GVIF Df GVIF^(1/(2*Df))\nSemantics                  1.148769  2        1.035281\nRecipient.type.bin         1.264991  1        1.124718\nTheme.type.bin             1.097579  1        1.047654\nRecipient.definiteness.bin 1.263261  1        1.123949\nTheme.definiteness.bin     1.613373  1        1.270186\nRecipient.animacy.bin      1.399355  1        1.182943\nLength.difference          1.420581  1        1.191881\n\n\nPartial effect plots on regression model\nThe effects package can help us shed a light on the reading of the coefficients in regression models. Here I plotted the partial effects for the predictors of the minimal model. For a detalied explanation, check out here.\n\n# partial effects plot\n# vertical axes plot probability of the predicted outcome\nplot(Effect(focal.predictors = c(\"Semantics\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.type.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.definiteness.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Theme.definiteness.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.animacy.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Length.difference\"), mod = trad_model_min))"
  },
  {
    "objectID": "MCL_seminar/study.html#minimal-regression-model",
    "href": "MCL_seminar/study.html#minimal-regression-model",
    "title": "Predicting the dative alternation in English",
    "section": "Minimal regression model",
    "text": "Minimal regression model\n\ntrad_model_min <- glmer(Response.variable ~ \n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin +\n              Recipient.animacy.bin+\n              Length.difference+\n              (1|Speaker.pruned)+ # random effect (intercept adjustment)\n              (1|Recipient.lemma.pruned)+\n              (1|Theme.lemma.pruned),\n              data = dat_us,\n              family=binomial,\n              glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 100000))\n)\nsummary(trad_model_min)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Response.variable ~ Semantics + Recipient.type.bin + Theme.type.bin +  \n    Recipient.definiteness.bin + Theme.definiteness.bin + Recipient.animacy.bin +  \n    Length.difference + (1 | Speaker.pruned) + (1 | Recipient.lemma.pruned) +  \n    (1 | Theme.lemma.pruned)\n   Data: dat_us\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e+05))\n\n     AIC      BIC   logLik deviance df.resid \n   283.9    344.7   -129.9    259.9     1158 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-29.3657   0.0115   0.0291   0.0750   3.2863 \n\nRandom effects:\n Groups                 Name        Variance Std.Dev.\n Theme.lemma.pruned     (Intercept) 4.05088  2.0127  \n Speaker.pruned         (Intercept) 0.47153  0.6867  \n Recipient.lemma.pruned (Intercept) 0.06079  0.2465  \nNumber of obs: 1170, groups:  \nTheme.lemma.pruned, 158; Speaker.pruned, 89; Recipient.lemma.pruned, 38\n\nFixed effects:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            0.1945     0.7095   0.274   0.7840    \nSemanticsC                            -1.0726     0.7750  -1.384   0.1664    \nSemanticsT                            -1.0996     0.5448  -2.018   0.0436 *  \nRecipient.type.binP                    2.8374     0.6030   4.705 2.54e-06 ***\nTheme.type.binP                       -0.9247     0.8281  -1.117   0.2642    \nRecipient.definiteness.binIndefinite  -1.5220     0.6707  -2.269   0.0233 *  \nTheme.definiteness.binIndefinite       2.6062     0.5973   4.363 1.28e-05 ***\nRecipient.animacy.binI                -2.5117     0.5917  -4.245 2.19e-05 ***\nLength.difference                     -1.9668     0.3820  -5.148 2.63e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) SmntcC SmntcT Rcp..P Thm..P Rcpnt.d.I Thm..I Rcpnt.n.I\nSemanticsC  -0.198                                                       \nSemanticsT  -0.217  0.318                                                \nRcpnt.typ.P -0.352 -0.130 -0.105                                         \nThm.typ.bnP -0.204  0.078 -0.091 -0.051                                  \nRcpnt.dfn.I -0.262  0.067  0.135  0.210  0.059                           \nThm.dfntn.I -0.463  0.039 -0.106  0.201  0.105 -0.277                    \nRcpnt.nmc.I -0.108  0.078  0.233 -0.134  0.015  0.266    -0.452          \nLngth.dffrn  0.192  0.041  0.041  0.146 -0.202  0.245    -0.426  0.315   \n\n\nModel summaries: Pseudo R2 measures (coefficient of determination) and C value\n\n# Pseudo R2 measures (see https://www.rdocumentation.org/packages/MuMIn/versions/1.40.4/topics/r.squaredGLMM)\n# R2m: Pseudo-R2 (marginal) -- represents the % variance explained by fixed factors\n# R2c: Pseudo-R2 (conditional) -- is interpreted as variance explained by both fixed agnd random factors (i.e. the entire model),\nr.squaredGLMM(trad_model_min)\n\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n\n\n                  R2m       R2c\ntheoretical 0.5980292 0.8320310\ndelta       0.2696375 0.3751436\n\n# Concordance index C\nsomers2(binomial()$linkinv(fitted(trad_model_min)), as.numeric(dat_us$Response.variable) -1)\n\n           C          Dxy            n      Missing \n   0.9898890    0.9797781 1170.0000000    0.0000000 \n\n\nPartial effect plots on regression model\n\n# partial effects plot (see https://data.library.virginia.edu/visualizing-the-effects-of-logistic-regression/)\n# vertical axes plot probability of the predicted outcome\nplot(Effect(focal.predictors = c(\"Semantics\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.type.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.definiteness.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Theme.definiteness.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Recipient.animacy.bin\"), mod = trad_model_min))\n\n\n\nplot(Effect(focal.predictors = c(\"Length.difference\"), mod = trad_model_min))"
  },
  {
    "objectID": "MCL_seminar/index.html#know-the-linguistic-phenomenon-you-want-to-analyse",
    "href": "MCL_seminar/index.html#know-the-linguistic-phenomenon-you-want-to-analyse",
    "title": "Predicting the dative alternation in English:",
    "section": "1. Know the linguistic phenomenon you want to analyse",
    "text": "1. Know the linguistic phenomenon you want to analyse\n…and how people how studied it before\nTry to look for:\n\nThe pivotal study on the phenomenon you want to study\nA couple of qualitative and quantitative investigations to get a sense of how researchers addressed it before you and what you can integrate to their research: a new approach, a new perspective, new data with a previous approach etc."
  },
  {
    "objectID": "MCL_seminar/index.html#know-your-dataset-and-your-data",
    "href": "MCL_seminar/index.html#know-your-dataset-and-your-data",
    "title": "Predicting the dative alternation in English:",
    "section": "2. Know your dataset (and your data)",
    "text": "2. Know your dataset (and your data)\n…especially if you did not build the dataset by yourselves!\n\nLinguistic information:\n\nwhere are the observations extracted from? and when?\ndo they belong to a specific language register?\ndid the authors apply some restrictions/limitations to the dataset?\n\nStatistical information:\n\nhow many observations the dataset counts originally and after restrictions\nhow many variables/predictors are annotated\n\nKeep always an eye on how much the dataset change after you apply your specific filters"
  },
  {
    "objectID": "MCL_seminar/index.html#understand-the-choice-of-the-analysis",
    "href": "MCL_seminar/index.html#understand-the-choice-of-the-analysis",
    "title": "Predicting the dative alternation in English:",
    "section": "3. Understand the choice of the analysis",
    "text": "3. Understand the choice of the analysis\n…based on the research questions and data you have\n\nWhy the authors chose to employ this/those analysis/es?\nWhich questions these analyses answer to? Do they address to specific perspectives of the linguistic phenomenon, or they are more general?\nDid these analyses bring an innovation in terms of methodologies employed in the field? Or they have been already used (see 1)?"
  },
  {
    "objectID": "MCL_seminar/index.html#know-your-dataset-and-your-data-1",
    "href": "MCL_seminar/index.html#know-your-dataset-and-your-data-1",
    "title": "Predicting the dative alternation in English:",
    "section": "2. Know your dataset (and your data)",
    "text": "2. Know your dataset (and your data)\n…especially if you did not build the dataset by yourselves!\n\nLanguage internal/external predictors: identify and describe them.\n\nFirst thing First: Response variable, type of the variables, levels of the variables\nFilters added in previous studies (they can drastically change the results of your replication study!!)\n\n\n\nt\nWe will see how the number of observation will change during the analysis of our case study."
  },
  {
    "objectID": "MCL_seminar/index.html#starting-point-szmrecsanyi-et-al.-2017",
    "href": "MCL_seminar/index.html#starting-point-szmrecsanyi-et-al.-2017",
    "title": "Predicting the dative alternation in English:",
    "section": "Starting point: Szmrecsanyi et al. (2017)",
    "text": "Starting point: Szmrecsanyi et al. (2017)\n\nThis paper investigates two of the well-known alternations in English, the dative and the genitive alternation in four varieties of spoken English. An ensemble of statistical analyses is employed to understand the extent to which the probabilistic grammar of genitive and dative variant choice differs across varieties.\n\n\n\nOur mini-replication study will focus only on Spoken American English, and only on the dative alternation\nGoal of my analysis is slightly different from the original one: to get familiar with the so-called traditional, top-down, manually annotated predictors for the dative alternation, and how well they predict the choice between the two variants in spoken American English.\n\n\n\n\nThis analysis was the first step in the first case study of my PhD research project."
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nA core section of every variationist research is the dataset and its annotation: Szmrecsanyi et al. (2017) presents two comprehensive and homogeneously manually annotated datasets for both alternations.\n\n\nThe paper offers a very, very good and detailed description of the dataset and its construction: read it to get an inspiration for your own study!\n\n\n\nVariationist sociolinguistics puts at the first place the choice for statistical and corpus-based methods in the exploration of language variation. Tagliamonte (2012)reported an immense and suggestive variety of quantitative approaches and statistical tools to use in our research."
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset-1",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset-1",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nLinguistic information\n\n\nThe dative tokens for American English were elicited from the Switchboard corpus of American English (Godfrey, Holliman & McDaniel 1992), as described in Bresnan et al. (2007). The Switchboard corpus covers telephone conversations collected at the beginning of the 1990s.\nThis dataset contains only observation with the verb give as verb of the dative construction.\nThe collection follows Bresnan et al. (2007) directions in defining interchangeable ditransitive and prepositional dative variants: only instances of the verb give with two argument Noun Phrases, with the exception of non-interchangeable contructions, were considered."
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset-2",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset-2",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nStatistical information\n\nThe original dative dataset counts 4136 observations, with the American English section counting 1190 observations, and a manual annotation for 25 predictors."
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset-3",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset-3",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nLanguage-external predictors\n\n\nVariety: in our case, US - one level\nSpeaker.ID\nSpeaker sex (only for a subset of observations)\nSpeaker year of birth (only for a subset of observations)"
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset-4",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset-4",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nLanguage-internal predictors: the authors annotated for well-known determinants of dative variation.\n\n\nResponse.variable: Ditransitive dative versus prepositional dative.\nRecipient/Theme.type: The annotation distinguishes between the following categories: (1) noun phrase; (2) personal pronoun; (3) demonstrative pronoun; (4) impersonal pronoun.\nRecipient/Theme.definiteness: The annotation distinguishes between the following categories: (1) definite; (2) indefinite (3) definite proper noun.\nRecipient/Theme.animacy: The annotation distinguishes between the following categories: (1) human and animal; (2) collective; (3) temporal; (4) locative; (5) inanimate.\nRecipient/Theme.length: Length of the recipient and theme phrases in orthographically transcribed words.\nSemantics (of dative verb): (1) transfer; (2) communication; (3) abstract.\nRecipient/Theme.head: Head lexeme of both the theme and the recipient.\n\n\n\n!! This is not an exhaustive list of the predictors annotated in the dataset, but are those which have been considered for the analysis."
  },
  {
    "objectID": "MCL_seminar/index.html#predicting-the-da-the-dataset-5",
    "href": "MCL_seminar/index.html#predicting-the-da-the-dataset-5",
    "title": "Predicting the dative alternation in English:",
    "section": "Predicting the DA: the dataset",
    "text": "Predicting the DA: the dataset\nLanguage-internal predictors: further manipulation\n\n\nReducing the predictors into binary contrasts: Recipient/Theme.type were reduced to pronominal ([2], [3], [4]) versus non-pronominal ([1]); Recipient/Theme.definiteness were reduced to definite ([1], [3]) versus indefinite ([2]); Recipient/Theme.animacy were reduced to animate ([1]) versus inanimate ([2], [3], [4], [5])\nCreating a the new predictor Length.difference: the Recipient/Theme.length measures were combined into a relative measure of length, calculated as log(Recipient.length) - log(Theme.length).\nRecipient/Theme.lemma: the annotated lemma of the heads.\n\n\n\nAnd now let’s go to the hands-on part!\n\n\n\n\nMethods is Corpus Linguistics - Leuven, 2022-12-8"
  }
]