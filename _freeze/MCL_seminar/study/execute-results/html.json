{
  "hash": "e3538fb8c1f0f7a08b9cc74302e5fe10",
  "result": {
    "markdown": "---\ntitle: \"Predicting the dative alternation in English\"\nsubtitle: \"A case study with logistic regression\"\nauthor: \"Chiara Paolini\"\nformat:\n  html:\n   echo: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\nHere is presented a step-by-step variation study on the English dative\nalternation based on Szmrecsanyi et al. (2017), using two traditional\nstatistical techniques in variationist analysis: *binomial logistic\nregression analysis* and *conditional random forest*.\n\nFor the theoretical explanation of the dataset and the techniques,\nplease check the **slides**.\n\n## Setup\n\nThe analysis needs the activation of different packages, `lme4` and\n`party` for regression modeling with random effects and conditional\nrandom forest respectively. `Hmisc`, `car` and `MuMIn` are pivotal for\ncalculating evaluation measures for regression modeling, while the\nlibrary `effects` allows the plot of partial effects plot.\n\n*Make sure to have them installed on your R studio version before\nrunning the script!*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)   # Easily Install and Load the 'Tidyverse'\nlibrary(here)        # Enable easy file referencing in project-oriented workflows\nlibrary(lme4)        # for mixed-effects regression\nlibrary(Hmisc)       # to calculate C values\nlibrary(car)         # to calculate VIFs\nlibrary(MuMIn)       # Multi-Model Inference for PSeudo R2 measures\nlibrary(effects)     # for partial effects plot\nlibrary(party)       # for ctrees and CRF\nlibrary(kableExtra)  # Construct Complex Table with 'kable' and Pipe Syntax\n```\n:::\n\n\n### Upload and filter the dataset\n\nThe dataset I will provide has already been manipulated: it represents\nonly the American subsection of the original dataset (*1190\nobservations*), plus it contains *recipient/theme.lemma*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_us <- read_tsv(here::here('MCL_seminar', 'data', 'dat_us.tsv'), show_col_types = F)\nhead(dat_us)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 42\n  Token…¹ Variety Corpus Speaker Speak…² Speak…³ Context Inclu…⁴ Inclu…⁵ Seman…⁶\n  <chr>   <chr>   <chr>    <dbl> <chr>     <dbl> <chr>   <chr>   <lgl>   <chr>  \n1 dat-26… US      SWBD      1002 F          1963 A: Rig… y       NA      T      \n2 dat-26… US      SWBD      1016 F          1945 being … y       NA      T      \n3 dat-26… US      SWBD      1026 F          1957 of tha… y       NA      A      \n4 dat-26… US      SWBD      1031 F          1940 but it… y       NA      A      \n5 dat-26… US      SWBD      1033 F          1965 B: Yea… y       NA      T      \n6 dat-26… US      SWBD      1050 M          1950 B: -- … y       NA      A      \n# … with 32 more variables: Response.variable <chr>, Verb.form <chr>,\n#   Verb.particle <lgl>, Recipient <chr>, Recipient.head <chr>,\n#   Recipient.type <chr>, Recipient.animacy <chr>,\n#   Recipient.definiteness <chr>, Recipient.length <dbl>,\n#   Recipient.coordinated <dbl>, Recipient.quantified.pronoun <dbl>,\n#   Recipient.contains.contraction <dbl>, Recipient.contains.acronym <dbl>,\n#   Recipient.contains.multiword.number <dbl>, …\n```\n:::\n:::\n\n\nAfter uploading the dataset, the second step is to filter and model the\ndataset based on our research needs. Specifically, here I first\n**select** only the predictors we are interested into and **filter out**\nthe observations with any NA values in the predictors and the ones that\ndo not contain the value **F** or **M** in the *speaker.sex* predictor.\nThen, I proceed to create the binary predictors as described in the\npaper, and the new *length.difference* variable by calculating the\ndifference between the log values of theme/recipient.length.I also did\nhere some level adjustment for the regression analysis: I set the\nreference level for the *response.variable* as **P**repositional, thus\nthe predicted odds will be for the **D**itransitive variant.\n*Speaker.pruned* and *Theme/recipient.lemma.pruned* are pruned variables\nto be used in the regression model as random effects (I will get into\ndetails in the regression section): **fct_lump_min** creates a new level\ncalled *other* for lemmas that appear fewer than 2 times. Eventually,\nthe character variables should be mutated in *factors*.\n\n> **IMPORTANT: Factors are used to represent categorical data in R**.\n> Factors are stored as integers, and have labels associated with these\n> unique integers (*levels*). While factors look (and often behave) like\n> character vectors, they are actually integers under the hood, and you\n> need to be careful when treating them like strings.\n>\n> Once created them using the `factor()` command, factors can only\n> contain a pre-defined set values, known as *levels*. By default, R\n> always sorts *levels* in alphabetical order.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_us <- dat_us %>% \n  select(Token.ID, Variety, Speaker, Speaker.sex, Response.variable,\n         Recipient.type, Theme.type,\n         Recipient.definiteness, Theme.definiteness,\n         Recipient.animacy, Theme.animacy,\n         Recipient.length, Theme.length,\n         Semantics,\n         Recipient.head, Theme.head, Recipient.lemma, Theme.lemma) %>% \n  na.omit() %>%\n  filter(Speaker.sex %in% c('F', 'M')) %>%\n  mutate(\n    Recipient.type.bin = if_else(Recipient.type == 'N', 'N', 'P'),\n    Theme.type.bin = if_else(Theme.type == 'N', 'N', 'P'),\n    Recipient.definiteness.bin = if_else(Recipient.definiteness == 'Indefinite', 'Indefinite', 'Definite'),\n    Theme.definiteness.bin = if_else(Theme.definiteness == 'Indefinite', 'Indefinite', 'Definite'),\n    Recipient.animacy.bin = if_else(Recipient.animacy == 'A', 'A', 'I'),\n    Theme.animacy.bin = if_else(Theme.animacy == 'A', 'A', 'I'),\n    Length.difference = log(Recipient.length) - log(Theme.length)) %>% \n  mutate(across(where(is.character), as.factor), \n    Speaker = factor(Speaker),\n    Response.variable = fct_relevel(Response.variable, \"P\"), #level the response variable: reference level is P-dative, thus predicted odds are for the D-dative\n    Speaker.pruned = fct_lump_min(Speaker, 5, other_level = \"other\"), #pruning the random effects\n    Theme.lemma.pruned = fct_lump_min(Theme.lemma, 2, other_level = \"other\"),\n    Recipient.lemma.pruned = fct_lump_min(Recipient.lemma, 2, other_level = \"other\")) \n head(dat_us)%>%\n  kbl(fixed_thead = T) %>%\n  kable_paper()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Token.ID </th>\n   <th style=\"text-align:left;\"> Variety </th>\n   <th style=\"text-align:left;\"> Speaker </th>\n   <th style=\"text-align:left;\"> Speaker.sex </th>\n   <th style=\"text-align:left;\"> Response.variable </th>\n   <th style=\"text-align:left;\"> Recipient.type </th>\n   <th style=\"text-align:left;\"> Theme.type </th>\n   <th style=\"text-align:left;\"> Recipient.definiteness </th>\n   <th style=\"text-align:left;\"> Theme.definiteness </th>\n   <th style=\"text-align:left;\"> Recipient.animacy </th>\n   <th style=\"text-align:left;\"> Theme.animacy </th>\n   <th style=\"text-align:right;\"> Recipient.length </th>\n   <th style=\"text-align:right;\"> Theme.length </th>\n   <th style=\"text-align:left;\"> Semantics </th>\n   <th style=\"text-align:left;\"> Recipient.head </th>\n   <th style=\"text-align:left;\"> Theme.head </th>\n   <th style=\"text-align:left;\"> Recipient.lemma </th>\n   <th style=\"text-align:left;\"> Theme.lemma </th>\n   <th style=\"text-align:left;\"> Recipient.type.bin </th>\n   <th style=\"text-align:left;\"> Theme.type.bin </th>\n   <th style=\"text-align:left;\"> Recipient.definiteness.bin </th>\n   <th style=\"text-align:left;\"> Theme.definiteness.bin </th>\n   <th style=\"text-align:left;\"> Recipient.animacy.bin </th>\n   <th style=\"text-align:left;\"> Theme.animacy.bin </th>\n   <th style=\"text-align:right;\"> Length.difference </th>\n   <th style=\"text-align:left;\"> Speaker.pruned </th>\n   <th style=\"text-align:left;\"> Theme.lemma.pruned </th>\n   <th style=\"text-align:left;\"> Recipient.lemma.pruned </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2626 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1002 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> T </td>\n   <td style=\"text-align:left;\"> us </td>\n   <td style=\"text-align:left;\"> something </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n   <td style=\"text-align:left;\"> something/pn </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> 1002 </td>\n   <td style=\"text-align:left;\"> something/pn </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2627 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1016 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> T </td>\n   <td style=\"text-align:left;\"> me </td>\n   <td style=\"text-align:left;\"> hers </td>\n   <td style=\"text-align:left;\"> me/pp </td>\n   <td style=\"text-align:left;\"> hers/pp </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> 1016 </td>\n   <td style=\"text-align:left;\"> other </td>\n   <td style=\"text-align:left;\"> me/pp </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2628 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1026 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> her </td>\n   <td style=\"text-align:left;\"> options </td>\n   <td style=\"text-align:left;\"> her/pp </td>\n   <td style=\"text-align:left;\"> option/nn </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> other </td>\n   <td style=\"text-align:left;\"> option/nn </td>\n   <td style=\"text-align:left;\"> her/pp </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2629 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1031 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> us </td>\n   <td style=\"text-align:left;\"> demos </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n   <td style=\"text-align:left;\"> demos/nn </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> other </td>\n   <td style=\"text-align:left;\"> other </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2630 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1033 </td>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> T </td>\n   <td style=\"text-align:left;\"> us </td>\n   <td style=\"text-align:left;\"> one </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n   <td style=\"text-align:left;\"> one/pn </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> 1033 </td>\n   <td style=\"text-align:left;\"> one/pn </td>\n   <td style=\"text-align:left;\"> us/pp </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dat-2631 </td>\n   <td style=\"text-align:left;\"> US </td>\n   <td style=\"text-align:left;\"> 1050 </td>\n   <td style=\"text-align:left;\"> M </td>\n   <td style=\"text-align:left;\"> D </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> me </td>\n   <td style=\"text-align:left;\"> shelter </td>\n   <td style=\"text-align:left;\"> me/pp </td>\n   <td style=\"text-align:left;\"> shelter/nn </td>\n   <td style=\"text-align:left;\"> P </td>\n   <td style=\"text-align:left;\"> N </td>\n   <td style=\"text-align:left;\"> Definite </td>\n   <td style=\"text-align:left;\"> Indefinite </td>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:left;\"> other </td>\n   <td style=\"text-align:left;\"> shelter/nn </td>\n   <td style=\"text-align:left;\"> me/pp </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## The bird's eye perspective: conditional random forest\n\nConditional random forest (CRF) is a multivariate statistical method\nthat can answer to the research question: ***which linguistic factors\nhelp to predict the use of particular linguistic variants?*** For an\naccessible introduction to conditional inference trees and random\nforest, check [Levshina\n(2020)](https://link.springer.com/chapter/10.1007/978-3-030-46216-1_25).\n\nCRF can be done using a great range of libraries (`partykit`, `ranger`\namong them): here I use `party`, the classic library for trees and\nforests in R.\n\nSince it is a partitioning algorithm, we need to set first a seed to get\nalways the same result. **forest** represents the implementation of the\nformula for the CRF using the\n[`cforest`](https://rdrr.io/cran/party/man/cforest.html) function:\nsimilarly to the formula for regression models, the syntax is:\n`cforest (response variable ~ [variables], data = data_name)` .\n\nAfter computing the CRF, `varimp` allows to compute variable importance\nmeasures: if `conditional = TRUE`, the importance of each variable is\ncomputed by adjuststing for correlations between predictor variables\n(*pretty obscure, better check the\n[documentation](https://rdrr.io/cran/party/man/varimp.html) for a longer\nand detailed explanation*).\n\nThe third chunk of the code is dedicated to the computation of the\n**Concordance index C-value**, better known as simply *C-value*, a\nnon-parametric measure of how well a statistical model fits a set of\nobservations. `Hmsic` is the reference library, using the function\n`somers2`.\n\n> C-value is a pivotal measure for most part of the statistical tools\n> employed in variationist analysis, especially for regression modeling.\n> However, make sure to not rely *only* on this measure for your\n> assessments as it gives a general evaluation of the model. For CRF is\n> fine, but we will see that it is not the case for regression modeling.\n\nFinally, it is possible to plot the result of the `varimp` computation\nand check for the most important variables in the prediction of the\nlinguistic variants. The red dotted line represents the threshold to\nconsider a variable slightly significant. For a detailed theoretical\nexplanation, see [Tagliamonte & Baayen\n(2012)](http://read.psych.uni-potsdam.de/pmr2/index.php?option=com_content&view=article&id=78:tagliamonte-and-baayen-2012-models-forests-and-trees-of-york-english-was-were-variation-as-a-case-study-for-statistical-practice&catid=24:publications&Itemid=32).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nforest = cforest(Response.variable ~ \n              Speaker.sex +\n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin+\n              Recipient.animacy.bin+\n              Theme.animacy.bin+\n              Length.difference,\n              data = dat_us)\n\n#### variable importance ranking, takes some time\nforest.varimp = varimp(forest, conditional = TRUE) \n\n#### model C index\n#### C ranges between 0 an 1; the closer to 1, the better the model\nprob2.rf <- unlist(treeresponse(forest))[c(FALSE, TRUE)]\nsomerssmallcrf <- somers2(prob2.rf, as.numeric(dat_us$Response.variable) - 1)\nsomerssmallcrf[\"C\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       C \n0.986687 \n```\n:::\n\n```{.r .cell-code}\n### the following code creates a dot plot visualizing the variable importance ranking\ndotplot(sort(forest.varimp), xlab=\"Variable Importance\", panel = function(x,y){\n  panel.dotplot(x, y, col='darkblue', pch=16, cex=1.1)\n  panel.abline(v=abs(min(forest.varimp)), col='red',\n               lty='longdash', lwd=2)\n}\n) \n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## The jeweler's eye perspective: regression modeling\n\nTo take a closer look at the predictors, and their direction in\npredicting one of the two linguistic variants, we use **binary logistic\nregression analysis with mixed effects** as implemented in the `lme4`\npackage in R.\n\nRegression modeling has in general a very simple and standardize code,\nbut it can be done in many different ways, using different techniques to\nget to the final results. Here I will show you the simplest bottom-up\ntechnique to get from a maximal model with all the predictors to a\nminimal model comprising of only the most meaningful predictors. The\nidea is to manually remove the predictor with the highest p-value at\neach run of the regression model, till we get a model with few\nmeaningful predictors.\n\n> There are many ways to implement automatic algorithms to get minimal\n> models by using *stepwise modeling.* However, I would recommend to use\n> those automatic techniques when you feel to master an advance\n> knowledge of your data and of regression modeling.\n\n### Maximal regression model\n\nThe reference level is **Prepositional**, thus the predicted odds are\nfor the **Ditransitive** alternation.\n\n`glmer` is the function to use for regression models with mixed-effects.\nSee the\n[documentation](https://www.rdocumentation.org/packages/lme4/versions/1.1-31/topics/glmer)\nfor the formula and an in-depth explanation of the code.\n\nHere the monthly-updated [GLMM\nbible](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html) for the\nthousands of problems in fitting the regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrad_model <- glmer(Response.variable ~ \n              Speaker.sex +\n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin+\n              Recipient.animacy.bin+\n              Theme.animacy.bin+\n              Length.difference+\n              (1|Speaker)+ # random effect (intercept adjustment)\n              (1|Recipient.lemma)+\n              (1|Theme.lemma),\n              data = dat_us,\n              family=binomial\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, :\nfailure to converge in 10000 evaluations\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in optwrap(optimizer, devfun, start, rho$lower, control = control, :\nconvergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n```{.r .cell-code}\nsummary(trad_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Response.variable ~ Speaker.sex + Semantics + Recipient.type.bin +  \n    Theme.type.bin + Recipient.definiteness.bin + Theme.definiteness.bin +  \n    Recipient.animacy.bin + Theme.animacy.bin + Length.difference +  \n    (1 | Speaker) + (1 | Recipient.lemma) + (1 | Theme.lemma)\n   Data: dat_us\n\n     AIC      BIC   logLik deviance df.resid \n   279.6    350.5   -125.8    251.6     1156 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.8673  0.0036  0.0122  0.0383  3.4311 \n\nRandom effects:\n Groups          Name        Variance  Std.Dev.\n Theme.lemma     (Intercept) 8.9752300 2.99587 \n Speaker         (Intercept) 0.0001514 0.01231 \n Recipient.lemma (Intercept) 0.2152492 0.46395 \nNumber of obs: 1170, groups:  \nTheme.lemma, 478; Speaker, 345; Recipient.lemma, 130\n\nFixed effects:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            0.7024     2.0973   0.335 0.737696    \nSpeaker.sexM                          -0.2330     0.5548  -0.420 0.674556    \nSemanticsC                            -1.2814     1.0826  -1.184 0.236554    \nSemanticsT                            -1.4212     0.7170  -1.982 0.047479 *  \nRecipient.type.binP                    2.8195     0.6920   4.074 4.61e-05 ***\nTheme.type.binP                       -1.0570     1.1745  -0.900 0.368147    \nRecipient.definiteness.binIndefinite  -2.4946     1.0636  -2.345 0.019006 *  \nTheme.definiteness.binIndefinite       3.1257     0.9373   3.335 0.000853 ***\nRecipient.animacy.binI                -3.4990     1.0314  -3.392 0.000693 ***\nTheme.animacy.binI                     0.4650     2.0049   0.232 0.816591    \nLength.difference                     -2.6538     0.6783  -3.912 9.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Spkr.M SmntcC SmntcT Rcp..P Thm..P Rcpnt.d.I Thm.d.I\nSpeaker.sxM -0.186                                                     \nSemanticsC  -0.099  0.045                                              \nSemanticsT  -0.268  0.204  0.298                                       \nRcpnt.typ.P -0.122  0.116 -0.135 -0.188                                \nThm.typ.bnP -0.146  0.155  0.084 -0.030 -0.004                         \nRcpnt.dfn.I -0.103  0.142  0.122  0.217  0.062  0.100                  \nThm.dfntn.I -0.104  0.044 -0.032 -0.224  0.261  0.066 -0.503           \nRcpnt.nmc.I -0.060  0.044  0.119  0.316 -0.191  0.051  0.552    -0.662 \nThm.nmcy.bI -0.877 -0.070 -0.018  0.089  0.015  0.006 -0.130     0.087 \nLngth.dffrn -0.018  0.035  0.094  0.243  0.027 -0.130  0.530    -0.665 \n            Rcpnt.n.I Thm.n.I\nSpeaker.sxM                  \nSemanticsC                   \nSemanticsT                   \nRcpnt.typ.P                  \nThm.typ.bnP                  \nRcpnt.dfn.I                  \nThm.dfntn.I                  \nRcpnt.nmc.I                  \nThm.nmcy.bI -0.149           \nLngth.dffrn  0.640    -0.083 \noptimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\nfailure to converge in 10000 evaluations\n```\n:::\n:::\n\n\n**Model summaries: Pseudo R2 measures (coefficient of determination), C\nvalue, and ViF**\n\nIn order to evaluate the performance of the model, it is possible to\ncompute different measures:\n\n-   **C-value** (see section on CRF)\n\n-   **Pseudo-R squared:** a goodness of fit measure explaining the\n    improvement in model likelihood over a null model (see for\n    discussion: [Hemmert et al.\n    (2018)](https://journals.sagepub.com/doi/abs/10.1177/0049124116638107?journalCode=smra#:~:text=LL%2Dbased%20pseudo%2DR2%20measures,indication%20of%20goodness%20of%20fit.)).\n    Here I compute the *marginal pseudo-R2* (the variance explained by\n    fixed factors) and the *conditional pseudo-R2* (variance explained\n    by both fixed and random factors (i.e. the entire model)).\n\n-   **Variance Inflation Factors:** A variance inflation factor (VIF)\n    detects multicollinearity in regression analysis. Multicollinearity\n    is when there is correlation between predictors (i.e. independent\n    variables) in a model; the presence of multicollinearity can\n    adversely affect your regression results. The VIF estimates how much\n    the variance of a regression coefficient is inflated due to\n    multicollinearity in the model. A goof VIF should be lower than 2.5,\n    but there is a lot of debate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R2\nr.squaredGLMM(trad_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'r.squaredGLMM' now calculates a revised statistic. See the help page.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                     R2m          R2c\ntheoretical 5.989478e-01 8.942824e-01\ndelta       4.138664e-15 6.179394e-15\n```\n:::\n\n```{.r .cell-code}\n# Concordance index C\nsomers2(binomial()$linkinv(fitted(trad_model)), as.numeric(dat_us$Response.variable) -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           C          Dxy            n      Missing \n   0.9967788    0.9935576 1170.0000000    0.0000000 \n```\n:::\n\n```{.r .cell-code}\n# Variance Inflation Factors\nvif(trad_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               GVIF Df GVIF^(1/(2*Df))\nSpeaker.sex                1.131815  1        1.063868\nSemantics                  1.298577  2        1.067498\nRecipient.type.bin         1.313491  1        1.146076\nTheme.type.bin             1.118899  1        1.057780\nRecipient.definiteness.bin 1.743511  1        1.320421\nTheme.definiteness.bin     2.526215  1        1.589407\nRecipient.animacy.bin      2.404110  1        1.550519\nTheme.animacy.bin          1.063911  1        1.031460\nLength.difference          2.439304  1        1.561827\n```\n:::\n:::\n\n\n### Minimal regression model\n\nAfter manually removing our not-significant predictors, we should get\nour minimal adequate regression model.\n\n> For the sake of length, I did not show every passage (I will show it\n> in class).\n\nBefore starting pruning the model, a good practice is to ***improve***\nthe regression model in two steps:\n\n-   By **pruning** the random effects (i.e. consider only the levels\n    higher that a certain threshold);\n\n-   By **optimizing the model**, using different techniques: one of the\n    most common is the optimizer *bobyqa* which enhances the performance\n    of the model together with *optCtrl = list(maxfun = 100000)* which\n    allows the model to perform more runs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrad_model_min <- glmer(Response.variable ~ \n              Semantics +\n              Recipient.type.bin +\n              Theme.type.bin +\n              Recipient.definiteness.bin +\n              Theme.definiteness.bin +\n              Recipient.animacy.bin+\n              Length.difference+\n              (1|Speaker.pruned)+ # random effect (intercept adjustment)\n              (1|Recipient.lemma.pruned)+\n              (1|Theme.lemma.pruned),\n              data = dat_us,\n              family=binomial,\n              glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 100000))\n)\nsummary(trad_model_min)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Response.variable ~ Semantics + Recipient.type.bin + Theme.type.bin +  \n    Recipient.definiteness.bin + Theme.definiteness.bin + Recipient.animacy.bin +  \n    Length.difference + (1 | Speaker.pruned) + (1 | Recipient.lemma.pruned) +  \n    (1 | Theme.lemma.pruned)\n   Data: dat_us\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 1e+05))\n\n     AIC      BIC   logLik deviance df.resid \n   283.9    344.7   -129.9    259.9     1158 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-29.3657   0.0115   0.0291   0.0750   3.2863 \n\nRandom effects:\n Groups                 Name        Variance Std.Dev.\n Theme.lemma.pruned     (Intercept) 4.05088  2.0127  \n Speaker.pruned         (Intercept) 0.47153  0.6867  \n Recipient.lemma.pruned (Intercept) 0.06079  0.2465  \nNumber of obs: 1170, groups:  \nTheme.lemma.pruned, 158; Speaker.pruned, 89; Recipient.lemma.pruned, 38\n\nFixed effects:\n                                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                            0.1945     0.7095   0.274   0.7840    \nSemanticsC                            -1.0726     0.7750  -1.384   0.1664    \nSemanticsT                            -1.0996     0.5448  -2.018   0.0436 *  \nRecipient.type.binP                    2.8374     0.6030   4.705 2.54e-06 ***\nTheme.type.binP                       -0.9247     0.8281  -1.117   0.2642    \nRecipient.definiteness.binIndefinite  -1.5220     0.6707  -2.269   0.0233 *  \nTheme.definiteness.binIndefinite       2.6062     0.5973   4.363 1.28e-05 ***\nRecipient.animacy.binI                -2.5117     0.5917  -4.245 2.19e-05 ***\nLength.difference                     -1.9668     0.3820  -5.148 2.63e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) SmntcC SmntcT Rcp..P Thm..P Rcpnt.d.I Thm..I Rcpnt.n.I\nSemanticsC  -0.198                                                       \nSemanticsT  -0.217  0.318                                                \nRcpnt.typ.P -0.352 -0.130 -0.105                                         \nThm.typ.bnP -0.204  0.078 -0.091 -0.051                                  \nRcpnt.dfn.I -0.262  0.067  0.135  0.210  0.059                           \nThm.dfntn.I -0.463  0.039 -0.106  0.201  0.105 -0.277                    \nRcpnt.nmc.I -0.108  0.078  0.233 -0.134  0.015  0.266    -0.452          \nLngth.dffrn  0.192  0.041  0.041  0.146 -0.202  0.245    -0.426  0.315   \n```\n:::\n:::\n\n\n**Model summaries: Pseudo R2 measures (coefficient of determination), C\nvalue and VIF**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pseudo R2 measures\nr.squaredGLMM(trad_model_min)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: the null model is correct only if all variables used by the original\nmodel remain unchanged.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                  R2m       R2c\ntheoretical 0.5980292 0.8320310\ndelta       0.2696375 0.3751436\n```\n:::\n\n```{.r .cell-code}\n# Concordance index C\nsomers2(binomial()$linkinv(fitted(trad_model_min)), as.numeric(dat_us$Response.variable) -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           C          Dxy            n      Missing \n   0.9898890    0.9797781 1170.0000000    0.0000000 \n```\n:::\n\n```{.r .cell-code}\n# Variance Inflation Factors\nvif(trad_model_min)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               GVIF Df GVIF^(1/(2*Df))\nSemantics                  1.148769  2        1.035281\nRecipient.type.bin         1.264991  1        1.124718\nTheme.type.bin             1.097579  1        1.047654\nRecipient.definiteness.bin 1.263261  1        1.123949\nTheme.definiteness.bin     1.613373  1        1.270186\nRecipient.animacy.bin      1.399355  1        1.182943\nLength.difference          1.420581  1        1.191881\n```\n:::\n:::\n\n\n### Partial effect plots on regression model\n\nThe `effects` package can help us shed a light on the reading of the\ncoefficients in regression models. Here I plotted the partial effects\nfor the predictors of the minimal model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# partial effects plot (see https://data.library.virginia.edu/visualizing-the-effects-of-logistic-regression/)\n# vertical axes plot probability of the predicted outcome\nplot(Effect(focal.predictors = c(\"Semantics\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(focal.predictors = c(\"Recipient.type.bin\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(focal.predictors = c(\"Recipient.definiteness.bin\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(focal.predictors = c(\"Theme.definiteness.bin\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-4.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(focal.predictors = c(\"Recipient.animacy.bin\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-5.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(Effect(focal.predictors = c(\"Length.difference\"), mod = trad_model_min))\n```\n\n::: {.cell-output-display}\n![](study_files/figure-html/unnamed-chunk-9-6.png){width=672}\n:::\n:::\n\n\n## References and further readings\n\n::: {#refs}\nBresnan, J., Cueni, A., Nikitina, T., Baayen, H., 2007. Predicting the\nDative Alternation, in: Bouma, G., Kraemer, I., Zwarts, J. (Eds.),\nCognitive Foundations of Interpretation. Royal Netherlands Academy of\nScience, Amsterdam, pp. 69--94.\n\nHemmert, G. A. J., Schons, L. M., Wieseke, J., & Schimmelpfennig, H.\n(2018). Log-likelihood-based Pseudo-R2 in Logistic Regression: Deriving\nSample-sensitive Benchmarks. *Sociological Methods & Research*, *47*(3),\n507--531.\n\nLevshina, N. (2020). Conditional Inference Trees and Random Forests. In:\nPaquot, M., Gries, S.T. (eds) A Practical Handbook of Corpus\nLinguistics. Springer, Cham., 611-44.\n\nSzmrecsanyi, B., Grafmiller, J., Bresnan, J., Rosenbach, A.,\nTagliamonte, S., Todd, S., 2017. Spoken syntax in a comparative\nperspective: The dative and genitive alternation in varieties of\nEnglish. Glossa J. Gen. Linguist. 2, 86.\n\nTagliamonte, S., & Baayen, R. H. (2012). Models, forests and trees of\nYork English: Was/were variation as a case study for statistical\npractice. *Language Variation and Change, 24*(2), 135--178.\n:::\n",
    "supporting": [
      "study_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}