{
  "hash": "1a0cf5a68eff4e04e30763add6d6acf9",
  "result": {
    "markdown": "---\ntitle: \"Using distributional semantics to annotate for semantic predictors\"\nsubtitle: \"A case study on the English dative alternation\"\nauthor:\n  - Chiara Paolini\n  - Benedikt Szmrecsanyi\n  - Mariana Montes\nformat:\n  revealjs:\n    footer: \"BKL - Liège, 2022-10-21\"\nexecute:\n  echo: false\ncss: styles.css\n---\n\n\n\n\n## Outline\n\n\n- Introduction: the dative alternation \n- From vectors to predictors \n- Forests and clouds \n- Insights and further research \n\n\n::: notes\nIn this talk, I would like to introduce you to the first case study of my PhD research project called \"How much does meaning matter? A fresh look at grammatical alternations\". The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study,and how semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors.\n:::\n\n# Introduction: the dative alternation\n\n## The dative alternation\n\n-   **Ditransitive** dative variant\n\n    \\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*my cousin*\\]~recipient~ \\[*some pizza*\\]~theme~\n\n-   **Prepositional** dative variant\n\n    \\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*some pizza*\\]~theme~ \\[*to my cousin*\\]~recipient~\n\n::: footer\nIntroduction: the dative alternation\n:::\n\n::: notes\nLet's dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as \"two or more constructions with a highly similar meaning - called linguistic variants - which represents a choice point for an individual speaker\".\n\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order). We chose this alternation for the immense amount of literature available (eg Bresnan et al 2007), and the available annotated data, like the dataset we used.\n:::\n\n## Modelling grammatical alternations\n\nIn previous literature, focus on three kinds of predictors:\n\n1.  **Formal predictors:** e.g., *structural complexity of constituents* (e.g., presence of heavy postmodification), *pronominality*, and *constituent length* (in words, syllables, or similar)\n\n2.  **Information status‐related predictors:** e.g., *givenness*\n\n3.  **Semantic, coarse-grained, higher-level predictors:** e.g., *animacy* (i.e., annotate for a binary distinction between animate and inanimate recipients -- see Bresnan et al. 2007)\n\n::: footer\nIntroduction: the dative alternation\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: notes\nTo explore the correlation of choices between the two variants, are usually modeled language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, traditional variationist approach is fairly good at manually annotating for formal predictors, such as (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy. This is because (next slide)\n:::\n\n## Role of semantic predictors in alternation predictions\n\n**Annotating for semantics is labor‐intensive and challenging to perform objectively.**\n\n::: {.caption style=\"font-size: 0.8em;\"}\n-   *What role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.*\n:::\n\n::: {style=\"text-align: center; color: red;\"}\n**We assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.**\n:::\n\n::: footer\nFrom vectors to predictors\n:::\n\n::: notes\nAccounting for the explanatory power of lexical meaning is not easy: Annotating for semantics is labor‐intensive and time-consuming, and it's challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover. (next slide)\n:::\n\n# From vectors to predictors\n\n## Automatically-generated semantic predictors\n\n::: {style=\"text-align: center; color: red;\"}\n**Our potential solution: automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).**\n:::\n\n-   New inputs from DMSs: **more data, less annotation**\n\n-   Methodological breakthrough: demonstrating to the research community how **distributional semantics methods can be marshaled to annotate for semantics** computationally, without the need for labor‐intensive hand‐coding.\n\n::: footer\nFrom vectors to predictors\n:::\n\n::: notes\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items.\n:::\n\n## Examples: recipient type-lemmas\n\nDAT-4100:\n\n-   ::: {style=\"font-size: 0.8em;\"}\n    \\[*if I*\\]~subject~ \\[*gave*\\]~verb~ \\[*it*\\]~theme~ \\[*to the government*\\]~recipient~ they would just waste it.\n    :::\n\nDAT-4067\n\n-   ::: {style=\"font-size: 0.8em;\"}\n    \\[*The judge*\\]~subject~ \\[*will usually, uh, give*\\]~verb~ \\[*custody*\\]~recipient~ \\[*to the mother*\\]~theme~ ninety-seven percent of the time.\n    :::\n\n::: footer\nFrom vectors to predictors\n:::\n\n::: notes\nRead the examples\n:::\n\n## Matrix of recipients\n\n| *Target*          | daughter/nn | europe/np | it/pp | dad/nn | troop/nn |\n|-------------------|-------------|-----------|-------|--------|----------|\n| **government/nn** | -1.23       | 3.23      | 0.21  | NA     | 2.59     |\n| **mother/nn**     | 4.36        | NA        | 1.65  | 2.89   | NA       |\n| **advance/nn**    | -2.32       | 2.09      | NA    | -0.59  | 3.67     |\n\n-   *Dative alternation dataset from Szmrecsanyi et al. (2017)*, which covers N = 4,136 dative observations in contemporary spoken English (Switchboard corpus)\n\n-   Corpus for building DMSs: *Corpus of Contemporary American English (COCA),* spoken register (ca. 127 million tokens)\n\n::: footer\nFrom vectors to predictors\n:::\n\n::: notes\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (show the DA slide). Let's consider the group of recipients, here exemplified by two real cases, government/nn and mother/nn. In this first part of the study, we implemented what we call a type-level model. As you can see in this table, the row in this table are word-type vectors, that is the distributional semantic vector as for ex. GOV/nn, is the results of the aggregation of all the individual occurrences in the corpus for the type lemma GOV/nn with each of the context words represented in the columns. The raw frequencies are then transformed, or better, weighted using association strength measures, such as PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\n\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters, such as context windows, PoS filters, length of the vectors, association and distance measures, and dimensionality reduction in order to compare those model and pick the best one BASED ON CUSTOMARY CRITERIA.\n\nOne parameter is missing, and it is the crucial one in order to obtain the predictors, that is the PAM clustering parameter.\n:::\n\n## Identifying the best model(s)\n\n**Outputs:**\n\n-   Type-level clusters = semantic predictors\n\n-   C-values for (binary) conditional inference trees and conditional random forests\n\n-   Percentage of negative silhouette in types and tokens (\\< = 0.25)\n\n**Best model:**\n\n-   **CS_4\\_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)**\n\n::: footer\nFrom vectors to predictors\n:::\n\n\n\n\n\n\n# Forests and clouds\n\n## From predictors to clouds: cloud of recipients\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.](index_files/figure-revealjs/fig-tsne-r-1.png){#fig-tsne-r width=960}\n:::\n:::\n\n\n::: footer\nForests and clouds\n:::\n\n::: notes\nBuilding the semantic predictors using DS means identifying cluster centers (called medoids) from the data and grouping the type-word vectors we obtained from the co-occurrences matrix around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\n\nThe best model is \\[...\\]\n\n\\[describe the clusters\\]\n:::\n\n## Cloud of themes\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of themes lemmas. Colors are clusters, labelled by their central members.](index_files/figure-revealjs/fig-tsne-t-1.png){#fig-tsne-t width=960}\n:::\n:::\n\n\n::: footer\nForests and clouds\n:::\n\n::: notes\n:::\n\n## Random forest of distributional predictors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/rf-dist-1.png){width=960}\n:::\n:::\n\n\n::: footer\nForests and clouds\n:::\n\n::: notes\n:::\n\n## Random forest of traditional and distributional predictors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/rf-full-1.png){width=960}\n:::\n:::\n\n\n::: footer\nForests and clouds\n:::\n\n::: notes\nSpeaker notes go here.\n:::\n\n# Insights and further research\n\n## Take-home messages\n\n::: footer\nInsights and further research\n:::\n\n::: notes\nSpeaker notes go here.\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}