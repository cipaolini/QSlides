{
  "hash": "ca40cd1d43c1eb082123efa12b8fc5c9",
  "result": {
    "markdown": "---\ntitle: \"Using distributional semantics to annotate for semantic predictors\"\nsubtitle: \"A case study on the English dative alternation\"\nauthor:\n  - Chiara Paolini\n  - Benedikt Szmrecsanyi\n  - Mariana Montes\nformat:\n  revealjs:\n    footer: \"BKL - Liège, 2022-10-21\"\nexecute:\n  echo: false\n---\n\n\n\n\n## Outline\n\n\n- Introduction: the dative alternation \n- From vectors to predictors \n- Forests and clouds \n- Insights and further research \n\n\n::: notes\nIn this talk, I would like to introduce you to the first case study of my PhD research project called \"How much does meaning matter? A fresh look at grammatical alternations\". The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study,and how semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors.\n:::\n\n# Introduction: the dative alternation\n\n## The dative alternation\n\n::: {style=\"color: #F39D41;\"}\n-   **Ditransitive** dative variant\n:::\n\n\\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*my cousin*\\]~recipient~ \\[*some pizza*\\]~theme~\n\n::: {style=\"color: #F39D41;\"}\n-   **Prepositional** dative variant\n:::\n\n\\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*some pizza*\\]~theme~ \\[*to my cousin*\\]~recipient~\n\n::: notes\nLet's dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as \"two or more constructions with a highly similar meaning - called linguistic variants - which represents a choice point for an individual speaker\".\n\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order). We chose this alternation for the immense amount of literature available (eg Bresnan et al 2007), and the available annotated data, like the dataset we used.\n:::\n\n## Modelling grammatical alternations\n\nIn previous literature, focus on three kinds of predictors:\n\n1.  **Formal predictors:** e.g., *structural complexity of constituents* (e.g., presence of heavy postmodification), *pronominality*, and *constituent length* (in words, syllables, or similar)\n\n2.  **Information status‐related predictors:** e.g., *givenness*\n\n3.  **Semantic, coarse-grained, higher-level predictors:** e.g., *animacy* (i.e., annotate for a binary distinction between animate and inanimate recipients -- see Bresnan et al. 2007)\n\n\n::: {.cell}\n\n:::\n\n\n::: notes\nTo explore the correlation of choices between the two variants, are usually modeled language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, traditional variationist approach is fairly good at manually annotating for formal predictors, such as (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy. This is because (next slide)\n:::\n\n## Role of semantic predictors in alternation predictions\n\n**Annotating for semantics is labor‐intensive and challenging to perform objectively.**\n\n::: {.caption style=\"font-size: 0.8em;\"}\n-   *What role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.*\n:::\n\n::: {style=\"text-align: center; color: #E04836;\"}\n**We assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.**\n:::\n\n::: notes\nAccounting for the explanatory power of lexical meaning is not easy: Annotating for semantics is labor‐intensive and time-consuming, and it's challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover. (next slide)\n:::\n\n# From vectors to predictors\n\n## Automatically-generated semantic predictors\n\n::: {style=\"text-align: center; color: #E04836;\"}\n**Our suggestion: automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).**\n:::\n\n-   New inputs from DMSs: **more data, less annotation**\n\n-   Methodological breakthrough: demonstrating to the research community how **distributional semantics methods can be marshaled to annotate for semantics** computationally, without the need for labor‐intensive hand‐coding.\n\n::: notes\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items.\n:::\n\n## Examples: recipient type-lemmas\n\n::: {style=\"color: #F39D41;\"}\n**DAT-4100**\n:::\n\n::: {style=\"font-size: 0.8em;\"}\n1.  \\[*if I*\\]~subject~ \\[*gave*\\]~verb~ \\[*it*\\]~theme~ \\[*to the government*\\]~recipient~ they would just waste it.\n:::\n\n::: {style=\"color: #F39D41;\"}\n**DAT-4067**\n:::\n\n::: {style=\"font-size: 0.8em;\"}\n2.  \\[*The judge*\\]~subject~ \\[*will usually, uh, give*\\]~verb~ \\[*custody*\\]~recipient~ \\[*to the mother*\\]~theme~ ninety-seven percent of the time.\n:::\n\n::: notes\nRead the examples\n:::\n\n## Matrix of recipients\n\n+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+\n|                               | ::: {style=\"color: #8D5924;\"} | ::: {style=\"color: #8D5924;\"} | ::: {style=\"color: #8D5924;\"} | ::: {style=\"color: #8D5924;\"} | ::: {style=\"color: #8D5924;\"} |\n|                               | daughter/nn                   | europe/np                     | it/pp                         | dad/nn                        | troop/nn                      |\n|                               | :::                           | :::                           | :::                           | :::                           | :::                           |\n+===============================+===============================+===============================+===============================+===============================+===============================+\n| ::: {style=\"color: #F39D41;\"} | -1.23                         | 3.23                          | 0.21                          | NA                            | 2.59                          |\n| **government/nn**             |                               |                               |                               |                               |                               |\n| :::                           |                               |                               |                               |                               |                               |\n+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| ::: {style=\"color: #F39D41;\"} | 4.36                          | NA                            | 1.65                          | 2.89                          | NA                            |\n| **mother/nn**                 |                               |                               |                               |                               |                               |\n| :::                           |                               |                               |                               |                               |                               |\n+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+\n| ::: {style=\"color: #F39D41;\"} | -2.32                         | 2.09                          | NA                            | -0.59                         | 3.67                          |\n| **advance/nn**                |                               |                               |                               |                               |                               |\n| :::                           |                               |                               |                               |                               |                               |\n+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+\n\n::: {style=\"font-size: 0.7em;\"}\n-   ***Dative alternation dataset from Szmrecsanyi et al. (2017)***, which covers **N = 4,136** dative observations in contemporary spoken English (Switchboard corpus)\n-   Corpus for building DMSs: ***Corpus of Contemporary American English** (COCA),* spoken register (ca. **127 million** tokens)\n:::\n\n::: notes\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (show the DA slide). Let's consider the group of recipients, here exemplified by two real cases, government/nn and mother/nn. In this first part of the study, we implemented what we call a type-level model. As you can see in this table, the row in this table are word-type vectors, that is the distributional semantic vector as for ex. GOV/nn, is the results of the aggregation of all the individual occurrences in the corpus for the type lemma GOV/nn with each of the context words represented in the columns. The raw frequencies are then transformed, or better, weighted using association strength measures, such as PPMI, that allow the model to bring up to the light the informative semantic relationships between the words.\n\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters, such as context windows, PoS filters, length of the vectors, association and distance measures, and dimensionality reduction in order to compare those model and pick the best one BASED ON CUSTOMARY CRITERIA.\n\nOne parameter is missing, and it is the crucial one in order to obtain the predictors, that is the PAM clustering parameter.\n:::\n\n\n\n\n\n# Forests and clouds\n\n## From predictors to clouds: cloud of recipients\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.](index_files/figure-revealjs/fig-tsne-r-1.png){#fig-tsne-r width=960}\n:::\n:::\n\n\n::: notes\nBuilding the semantic predictors using DS means identifying cluster centers (called medoids) from the data and grouping the type-word vectors we obtained from the co-occurrences matrix around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\n\nThe best model is CS_4\\_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction).\n\n\\[describe the clusters\\]\n:::\n\n## From predictor to clouds: cloud of themes\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of themes lemmas. Colors are clusters, labelled by their central members.](index_files/figure-revealjs/fig-tsne-t-1.png){#fig-tsne-t width=960}\n:::\n:::\n\n\n## From clouds to distributional semantic predictors\n\n::: {style=\"text-align: center; color: #E04836;\"}\nEach grouping of recipient/theme lemmas represents what we call ***distributional (semantic) predictor.***\n:::\n\nThe prediction of the dative variants is based on the **membership of the recipient/theme type-lemma in a particular semantic cluster.**\n\n::: notes\nIn what follows, we predict dative choices based on the membership of the recipients/themes in a particular semantic cluster.\n:::\n\n## Random forest of traditional and distributional predictors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/rf-full-1.png){width=960}\n:::\n:::\n\n\n::: notes\n-   Do not insist too much on the pronominality\n:::\n\n## Inside the most important clouds\n\n::: {style=\"color: #F39D41;\"}\n**Theme.one:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **one/pn**, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n:::\n\n::: {style=\"color: #F39D41;\"}\n**Recipient.everybody:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **everybody/pn**, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n:::\n\n::: {style=\"color: #F39D41;\"}\n**Recipient.it:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **it/pp**, that/dd, theory/nn, thing/nn\n:::\n\n## Another look at the clouds: regression modeling\n\n**c-values of regression model with only traditional and traditional+semantic predictors**\n\n|                                          | RM only trad predictors | RM trad+sem predictors |\n|------------------------------------------|-------------------------|------------------------|\n| **C-value for fixed effects**            | 0.9844213               | 0.9747162              |\n| **C-value for fixed and random effects** | 0.9854791               | 0.993123               |\n\n::: notes\nc-value (**Concordance index C)**: goodness of fit\n:::\n\n## Another look at the clouds: regression modeling\n\n**Difference of fixed/random effects between models**: in what extent the contribution of the predictors change in the models?\n\n| *Delta c-values*            | diffprop     | oddsratio | difflogits |\n|-----------------------------|--------------|-----------|------------|\n| **RM only trad predictors** | 0.007643931  | 2.127902  | 0.7551367  |\n| **RM trad+sem predictors**  | -0.009705108 | 0.6100782 | -0.4941681 |\n\n::: notes\nfrom Speelman\n:::\n\n\n\n\n\n# Insights and further research\n\n## Take-home messages\n\n::: {style=\"text-align: center; color: #E04836;\"}\n*\"You shall choose a variant by the company it keeps!\"*\n\n*(semi-cit. Firth 1955)*\n:::\n\n1.  Overall, there are some **distributional semantics clusters** that are **very predictive** of the alternation\n\n2.  Statistical analyses show how **traditional predictors outperform distributional semantics predictors** in terms of model performances\n\n::: notes\n2.  based on the c-value\n:::\n\n## Directions for future research\n\n-   **Other alternations:**\n    -   *Clausal complementation alternation* in the history of English\n\n    -   *Progressive alternation* in Italian\n-   **Experiment with token-level modeling**\n    -   see Montes (2021)\n\n::: notes\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}