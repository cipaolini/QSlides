{
  "hash": "400f61d3bbe5589872dd3f90c89d5e6b",
  "result": {
    "markdown": "---\ntitle: \"Annotation goes distributional:\"\nsubtitle: \"Modeling semantic predictors of the dative alternation using vector space models\"\nauthor:\n  - Chiara Paolini\nformat:\n  revealjs:\n    footer: \"QLVL colloquium - Leuven, 2022-11-4\"\nexecute:\n  echo: false\n---\n\n\n\n\n## Outline\n\n\n- Introduction: the dative alternation \n- From vectors to predictors \n- Forests and clouds \n- Insights and further research \n\n\n::: notes\nIn this talk, I would like to introduce you to the first case study of my PhD research project called \"How much does meaning matter? A fresh look at grammatical alternations\". The goal of this research is to examine if and how the way people choose between different ways of saying the same thing (i.e., grammatical alternations) depends on the meaning of the words in the utterance. I will start by introduce you to the dative alternation, our case study, and how lexical semantics is traditionally modeled in variationist linguistics. Then, I will illustrate our proposal, namely automatically-generated semantic predictors using DS techniques, and finally I will discuss the analyses and the results obtained from our first case study using type-level distributional semantic predictors.\n:::\n\n# Introduction: the dative alternation\n\n## The dative alternation\n\n::: {style=\"color: #F39D41;\"}\n1.  **a) Ditransitive** dative variant\n:::\n\n\\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*my cousin*\\]~recipient~ \\[*some pizza*\\]~theme~\n\n::: {style=\"color: #F39D41;\"}\n1.  **b) Prepositional** dative variant\n:::\n\n\\[*The waiter*\\]~subject~ \\[*gave*\\]~verb~ \\[*some pizza*\\]~theme~ \\[*to my cousin*\\]~recipient~\n\n::: notes\nLet's dive into our case study. The DA is one of the most investigated cases of grammatical alternation - where we define a GA as \"two or more constructions, called variants, with a highly similar meaning. An A represents choice point for the individual speaker\".\n\nIn English, there are two ways, two variants to encode the dative relation: the ditransitive dative construction (recipient-theme order), and the prepositional dative construction (theme-recipient order).\n:::\n\n## Modelling grammatical alternations\n\nIn previous literature, focus on three kinds of predictors:\n\n1.  **Formal predictors:** e.g., *structural complexity of constituents* (e.g., presence of heavy postmodification), *pronominality*, and *constituent length* (in words, syllables, or similar)\n\n2.  **Information status‐related predictors:** e.g., *givenness*\n\n3.  **Semantic, coarse-grained, higher-level predictors:** e.g., *animacy* (i.e., annotate for a binary distinction between animate and inanimate recipients -- see Bresnan et al. 2007)\n\n\n::: {.cell}\n\n:::\n\n\n::: notes\nTo explore the correlation of choices between the two variants are both implemented language-internal predictors as well as language-external predictors (such as sex, race/ethnicity, etc). Regarding the internal predictors, the traditional variationist approach is fairly good at manually annotating for formal predictors, such as in (1) and (2) for the dative alternation, but when it comes to the third point, namely the semantic predictors, the VA would annotate only for few semantic factors such as animacy. This is because (next slide)\n:::\n\n## Role of semantic predictors in alternation predictions\n\n**Annotating for semantics is labor‐intensive and challenging to perform objectively.**\n\n::: {.caption style=\"font-size: 0.8em;\"}\n-   *What role do semantic characteristics play in the choice of one of the two variants? At the current state of the research, we know very little about them.*\n:::\n\n::: {style=\"text-align: center; color: #E04836;\"}\n**We assume broad semantic equivalence of dative variants, but we are interested in extent to which semantics of materials in argument slots predicts choice.**\n:::\n\n::: notes\nAnnotating for semantics is labor‐intensive and time-consuming, and it's challenging to perform objectively and systematically. So if we ask ourselves: What role do semantic characteristics play in the choice of one of the two variants? We do not have a clear answer because we are missing those data. And this is the research gap we want to cover.\n\nIn particular, we are interested how much the semantic characteristics of the lexical material in the slots of the dative variants predict the choice.\n:::\n\n# From vectors to predictors\n\n## Automatically-generated semantic predictors\n\n::: {style=\"text-align: center; color: #E04836;\"}\n**Our suggestion: automatically-generated, corpus-based semantic predictors using distributional semantic models (DSMs).**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   New inputs from DMSs: **more data, less annotation**\n\n-   Distributional semantics is an usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different.\n:::\n\n::: notes\n(after reading the slides) DSMs can help us understanding and bring to the light the semantic characteristics of the lexical context in which those variants are embedded. In a nutshell, DS is a usage-based model of meaning, based on the assumption that items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. To do that, we operationalize the differences in the distribution of two (or more) items by extracting their co-occurrences from corpora: those differences can tell us something about the semantic relatedness of items.\n:::\n\n## Examples: recipient type-lemmas\n\n::: {style=\"color: #F39D41;\"}\n1.  **DAT-4100**\n:::\n\n::: {style=\"font-size: 0.8em;\"}\n-   \\[*if I*\\]~subject~ \\[*gave*\\]~verb~ \\[*it*\\]~theme~ \\[*to the government*\\]~recipient~ they would just waste it.\n:::\n\n::: {style=\"color: #F39D41;\"}\n2.  **DAT-4067**\n:::\n\n::: {style=\"font-size: 0.8em;\"}\n-   \\[*The judge*\\]~subject~ \\[*will usually, uh, give*\\]~verb~ \\[*custody*\\]~theme~ \\[*to the mother*\\]~recipient~ ninety-seven percent of the time.\n:::\n\n::: notes\nBut, what does it mean annotating predictors with DS? What we are going to distributionally model are the recipient and the theme of the alternation. (read the examples).\n:::\n\n## Matrix of recipients\n\n+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+\n|                                                | ::: {style=\"color: #8D5924; font-size: 35px;\"} | ::: {style=\"color: #8D5924; font-size: 35px;\"} | ::: {style=\"color: #8D5924; font-size: 35px;\"} | ::: {style=\"color: #8D5924; font-size: 35px;\"} | ::: {style=\"color: #8D5924; font-size: 35px;\"} |\n|                                                | daughter/nn                                    | europe/np                                      | it/pp                                          | dad/nn                                         | troop/nn                                       |\n|                                                | :::                                            | :::                                            | :::                                            | :::                                            | :::                                            |\n+================================================+================================================+================================================+================================================+================================================+================================================+\n| ::: {style=\"color: #F39D41; font-size: 35px;\"} | -1.23                                          | 3.23                                           | 0.21                                           | 0.0                                            | 2.59                                           |\n| **government/nn**                              |                                                |                                                |                                                |                                                |                                                |\n| :::                                            |                                                |                                                |                                                |                                                |                                                |\n+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+\n| ::: {style=\"color: #F39D41; font-size: 35px;\"} | 4.36                                           | 0.0                                            | 1.65                                           | 2.89                                           | 0.0                                            |\n| **mother/nn**                                  |                                                |                                                |                                                |                                                |                                                |\n| :::                                            |                                                |                                                |                                                |                                                |                                                |\n+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+\n| ::: {style=\"color: #F39D41; font-size: 35px;\"} | -2.32                                          | 2.09                                           | 0.0                                            | -0.59                                          | 3.67                                           |\n| **advance/nn**                                 |                                                |                                                |                                                |                                                |                                                |\n| :::                                            |                                                |                                                |                                                |                                                |                                                |\n+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+------------------------------------------------+\n\n::: {style=\"font-size: 0.7em;\"}\n-   ***Dative alternation dataset from Szmrecsanyi et al. (2017)***, which covers **N = 1,190** dative observations in contemporary **spoken American English** (Switchboard corpus)\n-   Corpus for building DMSs: ***Corpus of Contemporary American English** (COCA),* spoken register (ca. **127 million** tokens)\n:::\n\n::: notes\nIn this first part of the study, we implemented what we call a **type-level model**.\n\nLet's consider only the group of recipients, here exemplified by government/nn and mother/nn. As you can see in this co-occurence matrix, each row represents a target-words from the recipient slot: the aggregation of the frequencies between the TW and the CW constitutes a **word-type vector**. What you see here are raw frequencies transformed, or better, weighted using **association strength measures, such as PPMI**, that allow the model to bring up to the light the informative semantic relationships between the words.\n\nBuilding a DS model, means that we train a DS mode, a type-level one in this case, with different parameters and compare them to pick the best one BASED ON CUSTOMARY CRITERIA.\n\n(read about the data set)\n:::\n\n\n\n\n\n# Forests and clouds\n\n## Clouds of recipients\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of recipient lemmas. Colors are clusters, labelled by their central members.](index_files/figure-html/fig-tsne-r-1.png){#fig-tsne-r width=672}\n:::\n:::\n\n\n::: notes\nBuilding the semantic predictors using DS means identifying the central member of the cluster (called medoid) from the data and grouping the type-word vectors around them based on Euclidean distance metric. We provide the number of clusters the algorithm should create: 3, 8, 15 in our case.\n\nHere, a nice plot of the clusters, or clouds as Mariana Montes says, of the recipients in our best model. (The best model is CS_4\\_ol_10000_ppmi_10_cosine_k15 (with dimensionality reduction)).\n\n(describe the clusters)\n:::\n\n## Clouds of themes\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot of themes lemmas. Colors are clusters, labelled by their central members.](index_files/figure-html/fig-tsne-t-1.png){#fig-tsne-t width=672}\n:::\n:::\n\n\n## From clouds to distributional semantic predictors\n\n::: {style=\"text-align: center; color: #E04836;\"}\nEach grouping of recipient/theme lemmas represents what we call ***distributional (semantic) predictor.***\n:::\n\n::: {style=\"text-align: center;\"}\nThe prediction of the dative variants is based on the **membership of the recipient/theme type-lemma in a particular semantic cluster.**\n:::\n\n::: notes\nTo summarize, each grouping of recipient/theme lemmas represents what we call ***distributional (semantic) predictor.*** In what follows, we predict dative choices based on the membership of the recipients/themes in a particular semantic cluster by using two of the most classic variationist statistical tools of analysis, Conditional Random forest and Regression analysis.\n:::\n\n## Random forest\n\n::: {style=\"font-size: 0.6em;\"}\n**Random forest of traditional and distributional predictors**\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/rf-full-1.png){width=672}\n:::\n:::\n\n\n::: notes\nCRF ia a recursive partitioning method based on conditional inference trees: individual trees are 'grown', and their predictions are averaged. This statistical method can answer to the research question: which linguistic factors help to predict the use of particular linguistic variants? (explain the forest and show immediately the next slide) - Do not insist too much on the pronominality\n:::\n\n## Inside the most important clouds\n\n::: {style=\"color: #F39D41;\"}\n**Theme.one:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **one/pn**, everything/pn, feel/nn, it/pp, lot/rr, one/nn, poke/nn, something/pn, stuff/nn, tempt/vv, that/dd, them/pp, thing/nn, try/nn, way/nn\n:::\n\n::: {style=\"color: #F39D41;\"}\n**Recipient.everybody:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **everybody/pn**, anybody/pn, anyone/pn, anything/pn, everyone/pn, her/pp, him/pp, me/nn, me/pp, myself/pp, somebody/pn, stomach/nn, them/pp, us/pp, you/pp\n:::\n\n::: {style=\"color: #F39D41;\"}\n**Recipient.it:**\n:::\n\n::: {style=\"font-size: 0.7em;\"}\n-   **it/pp**, that/dd, theory/nn, thing/nn\n:::\n\n## Another look at the clouds: regression modelling\n\n::: {style=\"font-size: 0.7em;\"}\n**c-values of regression model with only traditional and traditional+semantic predictors**\n\n::: {style=\"font-size: 0.7em; text-align: center; color: #E04836;\"}\n**Chi-square (Χ^2^) goodness of fit (c-value)** = non-parametric measure of how well a statistical model fits a set of observations.\n:::\n:::\n\n+------------------------------------------+-------------------------+------------------------+\n|                                          | RM only trad predictors | RM trad+sem predictors |\n+==========================================+=========================+========================+\n| **C-value for fixed effects**            | 0.9844213               | 0.9747162              |\n+------------------------------------------+-------------------------+------------------------+\n| **C-value for fixed and random effects** | 0.9854791               | 0.993123               |\n+------------------------------------------+-------------------------+------------------------+\n\n::: notes\nWe also analyzed the distributional predictors using RM, adding them to the traditional, manually annotated, predictors. We are going to have a quick glance at the main results.\n\nWe computed the c-value (**Concordance index C: goodness of fit**) for the two models with fixed and random effects: even if the model with T+S performs better with the mixed effects, the higher c-v of 0.98 by the Tmodel suggests how the traditional p performs better alone, than in combination with the semantic ones.\n:::\n\n\n\n\n\n# Insights and further research\n\n## Take-home messages\n\n::: {style=\"text-align: center; color: #E04836;\"}\n*\"You shall choose a variant by the company it keeps!\"*\n\n*(semi-cit. Firth 1955)*\n:::\n\n1.  Overall, there are some **distributional semantics clusters** that are **very predictive** of the alternation\n\n2.  Statistical analyses show how **traditional predictors outperform distributional semantics predictors** in terms of model performances\n\n::: notes\n2.  based on the c-value\n:::\n\n## Directions for future research\n\n-   **Other alternations:**\n    -   *Clausal complementation alternation* in the history of English\n\n    -   *Progressive alternation* in Italian\n-   **Experiment with token-level modeling**\n    -   see Montes (2021)\n\n::: notes\nToken-level: in-depth analysis of the correlation of the single occurrences of the lemmas with the context\n:::\n\n## Thoughts? Comments? Suggestions?\n\n![](images/thank_you.png){fig-align=\"center\" width=\"677\"}\n\n::: {style=\"text-align: center; font-size: 0.6em;\"}\n***chiara.paolini\\@kuleuven.be***\n:::\n\n## Traditional predictors {visibility=\"uncounted\"}\n\n::: {style=\"font-size: 0.7em;\"}\n-   **Recipient/Theme.type:** The annotation distinguishes between the following categories: (1) noun phrase; (2) personal pronoun; (3) demonstrative pronoun; (4) impersonal pronoun.\n\n-   **Recipient/Theme.definiteness:** The annotation distinguishes between the following categories: (1) definite; (2) indefinite (3) definite proper noun.\n\n-   **Recipient/Theme.animacy:** The annotation distinguishes between the following categories: (1) human and animal; (2) collective; (3) temporal; (4) locative; (5) inanimate.\n\n-   **Length.difference:** The log difference between recipient and theme lengths (see Bresnan & Ford 2010).\n\n-   **Semantics (of dative verb):** (1) transfer; (2) communication; (3) abstract.\n\n-   **Recipient/Theme.head:** Head lexeme of both the theme and the recipient.\n:::\n\n## Inside the recipients clouds {visibility=\"uncounted\"}\n\n::: {style=\"font-size: 0.7em;\"}\n-   **Recipient.advance**: advance/vv, america/np, arm/nn, country/nn, europe/np, government/nn, iranian/nn, nation/nn, nicaragua/np, russia/np, schwartzkopf/np, troop/nn, vietnamese/np\n\n-   **Recipient.wife**: actress/nn, wife/nn, artist/nn, boss/nn, brother/nn, daughters/nn, friend/nn, husband/nn, kitty/np, ryan/np, sister/nn, thomases/np, trek/np\n\n-   Recipient.kids: agencies/nn\n\n    2\n\n    0.3443036000\n\n    kids/nn\n\n    16\n\n    boys/nn\n\n    2\n\n    0.5979984000\n\n    kids/nn\n\n    20\n\n    businesses/nn\n\n    2\n\n    0.4080819000\n\n    kids/nn\n\n    29\n\n    companies/nn\n\n    2\n\n    0.5218712000\n\n    kids/nn\n\n    33\n\n    countries/nn\n\n    2\n\n    0.5652321000\n\n    kids/nn\n\n    37\n\n    criminals/nn\n\n    2\n\n    0.3794335700\n\n    kids/nn\n\n    42\n\n    dogs/nn\n\n    2\n\n    0.5007423000\n\n    kids/nn\n\n    43\n\n    employees/nn\n\n    2\n\n    0.4876230700\n\n    kids/nn\n\n    56\n\n    friends/nn\n\n    2\n\n    0.4518532500\n\n    kids/nn\n\n    60\n\n    grandparents/nn\n\n    2\n\n    0.3280267400\n\n    kids/nn\n\n    66\n\n    guys/nn\n\n    2\n\n    0.3701141800\n\n    kids/nn\n\n    70\n\n    horses/nn\n\n    2\n\n    0.3475765000\n\n    kids/nn\n\n    72\n\n    houses/nn\n\n    2\n\n    0.5975894300\n\n    kids/nn\n\n    82\n\n    kids/nn\n\n    2\n\n    0.6238054000\n\n    kids/nn\n\n    95\n\n    members/nn\n\n    2\n\n    0.4905860400\n\n    kids/nn\n\n    104\n\n    parents/nn\n\n    2\n\n    0.5942958000\n\n    kids/nn\n\n    113\n\n    rashad/np\n\n    2\n\n    0.3486836900\n\n    kids/nn\n\n    124\n\n    sons/nn\n\n    2\n\n    0.5616968000\n\n    kids/nn\n\n    127\n\n    students/nn\n\n    2\n\n    0.4786918000\n\n    kids/nn\n\n    135\n\n    things/nn\n\n    2\n\n    0.4555949600\n\n    kids/nn\n\n    141\n\n    twins/nn\n\n    2\n\n    0.5751200300\n\n    kids/nn\n\n    152\n\n    women/nn\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}